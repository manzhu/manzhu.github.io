<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>轻松创建 docker image</title>
    <url>/cn/build-docker-image/</url>
    <content><![CDATA[<p>在 Docker 技术出现之前, 搭建 lab 环境复现问题实在是件非常让人头疼的事情. 按照传统的方式搭建 LAB 环境非常麻烦. 比如你需要测试 nginx 或是 ftp, 我们需要安装 linux 服务器, 配置网络, 分配 IP, 更新软件源, 安装软件包, 调整配置文件… 而 Docker 的出现彻底解放了这些繁琐的工作! 通过一个 Dockerfile, 用户可以build 出符合自己需要的 docker image 并在几分钟内拉起一个服务.</p>
<span id="more"></span>

<h2 id="Dockerfile"><a href="#Dockerfile" class="headerlink" title="Dockerfile"></a>Dockerfile</h2><p>要创建自己的 docker image, 最重要的就是这个 <code>Dockerfile</code> 啦! Dockerfile 是一个纯文本文件, 里面包含了创建 docker image 的各种”指令”. 简单来说docker 会一步步执行 Dockerfile 中的命令来完成一个image 的构建. 比如我们来看一个简单的 Dockerfile:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FROM ubuntu:18.04</span><br><span class="line">RUN apt-get -yqq update &amp;&amp; apt-get -yqq install nginx </span><br><span class="line">RUN mkdir -p /var/www/html/website</span><br><span class="line">ADD nginx/global.conf /etc/nginx/conf.d/</span><br><span class="line">ADD nginx/nginx.conf /etc/nginx/nginx.conf</span><br><span class="line">EXPOSE 80</span><br><span class="line">CMD /bin/sh -c nginx</span><br></pre></td></tr></table></figure>

<p>其实不用过多解释大家应该基本也能理解这些语句的含义:</p>
<ul>
<li>FROM ubuntu:18.04: 以官方的ubuntu 18.04 为模板</li>
<li>RUN apt-get -yqq update &amp;&amp; apt-get -yqq install nginx:  执行 apt 更新并且安装 nginx 软件包</li>
<li>RUN mkdir -p /var/www/html/website: 在 docker image 中创建存放站点的目录</li>
<li>ADD nginx/global.conf /etc/nginx/conf.d/: 当前 Dockerfile 同级目录中的 nginx/global.conf 放入 container 中的 /etc/nginx/conf.d/</li>
<li>ADD nginx/nginx.conf /etc/nginx/nginx.conf: 当前目录中子目录nginx/nginx.conf 覆盖 container 中的/etc/nginx/nginx.conf</li>
<li>EXPOSE 80: container 开启 80 端口接收外部的访问</li>
</ul>
<p>在 nginx/nginx.conf 中是和 nginx server 本身相关的配置参数. 在其中比较重要的是 <code>daemon off;</code> 这个参数使得nginx 保持在container 的前端运行, 从而确保服务不被终止.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ cat nginx.conf</span><br><span class="line">user www-data;</span><br><span class="line">worker_processes 4;</span><br><span class="line">pid /run/nginx.pid;</span><br><span class="line">daemon off;</span><br><span class="line"></span><br><span class="line">events &#123;  &#125;</span><br><span class="line"></span><br><span class="line">http &#123;</span><br><span class="line">  sendfile on;</span><br><span class="line">  tcp_nopush on;</span><br><span class="line">  tcp_nodelay on;</span><br><span class="line">  keepalive_timeout 65;</span><br><span class="line">  types_hash_max_size 2048;</span><br><span class="line">  include /etc/nginx/mime.types;</span><br><span class="line">  default_type application/octet-stream;</span><br><span class="line">  access_log /var/log/nginx/access.log;</span><br><span class="line">  error_log /var/log/nginx/error.log;</span><br><span class="line">  gzip on;</span><br><span class="line">  gzip_disable &quot;msie6&quot;;</span><br><span class="line">  include /etc/nginx/conf.d/*.conf;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在 nginx/global.conf 中, 是我们网站的配置. 可以看到我们将网站的 root 指定为在 Dockerfile 中创建的目录 /var/www/html/website, 并且也指定了对应的日志目录. </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">        listen          0.0.0.0:80;</span><br><span class="line">        server_name     _;</span><br><span class="line"></span><br><span class="line">        root            /var/www/html/website;</span><br><span class="line">        index           index.html index.htm;</span><br><span class="line"></span><br><span class="line">        access_log      /var/log/nginx/default_access.log;</span><br><span class="line">        error_log       /var/log/nginx/default_error.log;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Build"><a href="#Build" class="headerlink" title="Build"></a>Build</h2><p>完成了上面的准备工作后, 我们就可以开始 build 阶段的工作啦!  这一步只需要一条命令即可, 不过一定记得执行的时候一定切换到 Dockerfile 文件所在的目录中哦! 另外命令的最后有一个点表示 Dockerfile的路径, 千万别漏啦.(当然你也可以写 Dockerfile 的绝对路径. )</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker build -t &quot;m4n/nginx_demo&quot; .</span><br></pre></td></tr></table></figure>

<p>随后docker 就会依据你的 Dockerfile 中的指令一步一步完成 image 的打包操作. 很快你的 image 就制作完毕了.</p>
<h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><p>image 制作完毕后, 我们可以试试看用它拉起一个镜像:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ docker run -d -p 80 --name first_site -v $PWD/website:/var/www/html/website m4n/nginx_demo nginx</span><br></pre></td></tr></table></figure>

<p>我们来了解下这条命令中的几个参数:</p>
<ul>
<li>-d 后台运行容器  </li>
<li>-p: 端口映射. 比如 -p 3333:80 指定主机的3333端口映射到容器的80端口; 直接写也表示直接将宿主机的 80 端口映射到容器的端口 80  </li>
<li>-v 本地目录:容器目录. 将宿主机本地目录中的 website 链接到容器中的 /var/www/html/website 这个目录中. ( /var/www/html/website 就是上面 docker image 中的 nginx 存放站点文件的目录) 这种挂载卷的方式打通了宿主机与容器间的文件共享. 这样我们就可以在编辑本地website文件夹中的index.html内容的同时, 使得container 里的nginx 也能感知到这个变化. </li>
</ul>
<p>当然爱动脑筋的同学肯定想到了, 如果要对 nginx.conf 或者 global.conf 进行编辑, 只需要将它们与容器中的文件通过参数 -v 进行映射即可.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run --name nginx -p 80:80 -v nginx/nginx.conf:/etc/nginx/nginx.conf -v /var/log:/var/log/nginx -v nginx/global.conf:/etc/nginx/conf.d/global.conf -d nginx</span><br></pre></td></tr></table></figure>

<p>Dockerfile 的编写还有许多用法, 具体可以参考<a href="https://docs.docker.com/engine/reference/builder/">Dockerfile reference</a></p>
<script type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.7.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.7.0/dist/mindmap.min.css">]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>build</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes 服务发布之 F5-BIGIP</title>
    <url>/cn/cis/</url>
    <content><![CDATA[<p>在 k8s 的世界中, 应用的一生可能稍纵即逝. 一个 pod 的存活时间可能只有几个小时甚至几分钟. 在这个快速变化的世界中里, Ingress 就像一位忙碌的交通警察默默地控制着集群内外流量的发布, 回收. 说到 Kubernetes Ingress 大家比较熟悉的是 nginx 或者是 envoy 这类基于软件实现的解决方案. 其实 Ingress 也可以基于硬件来实现. 硬件平台本身在流量的处理性能以及功能扩展上还是有软件无法相比的优势, 比如 IPV6/IPV4的转换, 基于硬件芯片的 ssl 加速处理能力, 基于硬件的 DDOS 防护以及应用安全等方面都提供了相对成熟, 易管理的方案. 下面这个例子中我们将一台 BIGIP 加入 k8s 作为其 ingress service 来为集群中的应用发布服务.</p>
<span id="more"></span>

<p>F5 通过官方发布的 CIS 来实现 BIG-IP 与 k8s 集群的整合. CIS 的全称是 F5 BIG-IP Container Ingress Services, 之前也被称为 BIGIP Controller.  它在 k8s 中作为一个 pod 的形式出现, 其作用主要负责监控 k8s 中 service/pod 的变化并将其发布到 BIGIP 上. CIS 以 NodePort 或者是 ClusterIP的方式在 BIG-IP 上创建 monitor, pool, virtual server 来发布服务. </p>
<h2 id="NodePort"><a href="#NodePort" class="headerlink" title="NodePort"></a>NodePort</h2><p>通过 NodePort 发布应用时, BIG-IP 会将流量转发到 Node 上暴露的 NodePort 端口 (32771), 流量进入 Node 的 IP 10.1.10.100 与 10.1.10.200 后, 通过 kube-proxy 转发到对应的 pod 成员中(10.244.1.2 与 10.244.2.2). 所以在 NodePort 模式中, BIG-IP 中看到的 pool member IP 是 Node 的 IP 和端口.  </p>
<p>如下图(转自官网):</p>
<div style="width:60%;margin:auto" align="center"><img src="/cn/cis/NodePort.png" class=""></div>

<p>下面我们就来将一台 BIG-IP 整合到 k8s中:</p>
<ul>
<li><p>在 BIG-IP 上创建partition: k8s</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tmsh create auth partition k8s</span><br></pre></td></tr></table></figure></li>
<li><p>在 k8s 中创建 BIG-IP 的登陆账号. 用户名: admin/密码: admin</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create secret generic bigip-login -n kube-system --from-literal=username=admin --from-literal=password=admin</span><br></pre></td></tr></table></figure></li>
<li><p>创建安装 CIS 需要使用的 serviceaccount:</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create serviceaccount bigip-ctlr -n kube-system</span><br></pre></td></tr></table></figure></li>
<li><p>创建 ClusterRole 为 serviceaccount 分配权限: k8s_rbac_yaml</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># for use in k8s clusters only</span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">name: bigip-ctlr-clusterrole</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;&quot;, &quot;extensions&quot;, &quot;networking.k8s.io&quot;]</span><br><span class="line">resources: [&quot;nodes&quot;, &quot;services&quot;, &quot;endpoints&quot;, &quot;namespaces&quot;, &quot;ingresses&quot;, &quot;pods&quot;, &quot;ingressclasses&quot;]</span><br><span class="line">verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]</span><br><span class="line">- apiGroups: [&quot;&quot;, &quot;extensions&quot;, &quot;networking.k8s.io&quot;]</span><br><span class="line">resources: [&quot;configmaps&quot;, &quot;events&quot;, &quot;ingresses/status&quot;, &quot;services/status&quot;]</span><br><span class="line">verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;, &quot;create&quot;, &quot;patch&quot;]</span><br><span class="line">- apiGroups: [&quot;cis.f5.com&quot;]</span><br><span class="line">resources: [&quot;virtualservers&quot;,&quot;virtualservers/status&quot;, &quot;tlsprofiles&quot;, &quot;transportservers&quot;, &quot;ingresslinks&quot;, &quot;externaldnss&quot;]</span><br><span class="line">verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;, &quot;patch&quot;]</span><br><span class="line">- apiGroups: [&quot;fic.f5.com&quot;]</span><br><span class="line">resources: [&quot;f5ipams&quot;, &quot;f5ipams/status&quot;]</span><br><span class="line">verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;, &quot;create&quot;, &quot;patch&quot;, &quot;delete&quot;]</span><br><span class="line">- apiGroups: [&quot;apiextensions.k8s.io&quot;]</span><br><span class="line">resources: [&quot;customresourcedefinitions&quot;]</span><br><span class="line">verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;, &quot;create&quot;, &quot;patch&quot;]</span><br><span class="line">- apiGroups: [&quot;&quot;, &quot;extensions&quot;]</span><br><span class="line">resources: [&quot;secrets&quot;]</span><br><span class="line">verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">name: bigip-ctlr-clusterrole-binding</span><br><span class="line">namespace: kube-system</span><br><span class="line">roleRef:</span><br><span class="line">apiGroup: rbac.authorization.k8s.io</span><br><span class="line">kind: ClusterRole</span><br><span class="line">name: bigip-ctlr-clusterrole</span><br><span class="line">subjects:</span><br><span class="line">- apiGroup: &quot;&quot;</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">name: bigip-ctlr</span><br><span class="line">namespace: kube-system</span><br></pre></td></tr></table></figure></li>
<li><p>应用 YAML 完成 Cluster Role 与 serviceaccounts 的绑定</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f  k8s_rbac.yaml</span><br></pre></td></tr></table></figure></li>
<li><p>安装 CIS: cis_nodeport.yaml. 这里需要注意修改 <code>--bigip-url</code> 将其指向 BIG-IP management IP 地址. <code>--bigip-partition</code> 中指定 BIG-IP 上的 partition 名称, 在这里是 k8s. 另外需要注意的是 <code>--pool-member-type</code> 是 nodeport.</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">name: k8s-bigip-ctlr-deployment</span><br><span class="line">namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line"># DO NOT INCREASE REPLICA COUNT</span><br><span class="line">replicas: 1</span><br><span class="line">selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">    app: k8s-bigip-ctlr-deployment</span><br><span class="line">template:</span><br><span class="line">    metadata:</span><br><span class="line">    labels:</span><br><span class="line">        app: k8s-bigip-ctlr-deployment</span><br><span class="line">    spec:</span><br><span class="line">    # Name of the Service Account bound to a Cluster Role with the required</span><br><span class="line">    # permissions</span><br><span class="line">    containers:</span><br><span class="line">        - name: k8s-bigip-ctlr</span><br><span class="line">        image: &quot;f5networks/k8s-bigip-ctlr:latest&quot;</span><br><span class="line">        env:</span><br><span class="line">            - name: BIGIP_USERNAME</span><br><span class="line">            valueFrom:</span><br><span class="line">                secretKeyRef:</span><br><span class="line">                # Replace with the name of the Secret containing your login</span><br><span class="line">                # credentials</span><br><span class="line">                name: bigip-login</span><br><span class="line">                key: username</span><br><span class="line">            - name: BIGIP_PASSWORD</span><br><span class="line">            valueFrom:</span><br><span class="line">                secretKeyRef:</span><br><span class="line">                # Replace with the name of the Secret containing your login</span><br><span class="line">                # credentials</span><br><span class="line">                name: bigip-login</span><br><span class="line">                key: password</span><br><span class="line">        command: [&quot;/app/bin/k8s-bigip-ctlr&quot;]</span><br><span class="line">        args: [</span><br><span class="line">            # See the k8s-bigip-ctlr documentation for information about</span><br><span class="line">            # all config options</span><br><span class="line">            # https://clouddocs.f5.com/containers/latest/</span><br><span class="line">            &quot;--bigip-username=$(BIGIP_USERNAME)&quot;,</span><br><span class="line">            &quot;--bigip-password=$(BIGIP_PASSWORD)&quot;,</span><br><span class="line">            &quot;--bigip-url=&lt;ip_address-or-hostname&gt;&quot;,</span><br><span class="line">            &quot;--bigip-partition=&lt;name_of_partition&gt;&quot;,</span><br><span class="line">            &quot;--pool-member-type=nodeport&quot;,</span><br><span class="line">            &quot;--insecure&quot;,</span><br><span class="line">            ]</span><br><span class="line">    serviceAccountName: bigip-ctlr</span><br></pre></td></tr></table></figure></li>
<li><p>应用 YAML 完成 CIS 部署</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f  cis_nodeport.yaml</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="ClusterIP"><a href="#ClusterIP" class="headerlink" title="ClusterIP"></a>ClusterIP</h2><p>通过 ClusterIP 发布应用时, BIGIP 将流量封装在 VXLAN 的tunnel 中并直接转发到 pod. 通过这种方式, 应用流量无需通过 kube-proxy 进行二次转发因此更加高效, 所以在 BIGIP 上我们看到 pool member 的地址是 pod 的 IP 地址. </p>
<div style="width:60%;margin:auto" align="center"><img src="/cn/cis/ClusterIP.png" class=""></div>

<p>BIG-IP 可以直接得知 pod 中两个 container 实例的 IP 地址: 10.244.1.2 和 10.244.2.2, 同时流量也不用再通过 kube-proxy 进行转发. </p>
<p>部署 Cluster IP 模式的 CIS 步骤如下:</p>
<ul>
<li><p>在 BIG-IP 上创建 partition: k8s</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tmsh create auth partition k8s</span><br></pre></td></tr></table></figure></li>
<li><p>在 BIG-IP 上创建 VXLAN Tunnel:</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tmsh create net self 172.16.1.28/16 &#123; allow-service all vlan Internal &#125; </span><br><span class="line">tmsh create net tunnels vxlan fl-vxlan port 8472 flooding-type none </span><br><span class="line">tmsh create net tunnels tunnel flannel_vxlan key 1 profile fl-vxlan local-address 172.16.1.28 </span><br><span class="line">tmsh create net self 10.244.100.3 address 10.244.100.3/16 allow-service none vlan flannel_vxlan </span><br></pre></td></tr></table></figure>
<p>  其中 172.16.1.28 是 BIG-IP 在 K8S 集群中的 Node IP 地址, 用来和集群中的其他 Node 进行通讯; 10.244.100.3 则是 pod 的 ip 地址, 用来和 pod 里的container 进行通讯.</p>
</li>
<li><p>在 k8s 中创建 BIG-IP 的登陆账号. 用户名: admin/密码: admin</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create secret generic bigip-login -n kube-system --from-literal=username=admin --from-literal=password=admin</span><br></pre></td></tr></table></figure></li>
<li><p>创建安装 CIS 需要使用的 serviceaccount:</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create serviceaccount bigip-ctlr -n kube-system</span><br></pre></td></tr></table></figure></li>
<li><p>创建 ClusterRole 为 serviceaccount 分配权限: k8s_rbac_yaml</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># for use in k8s clusters only</span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">name: bigip-ctlr-clusterrole</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;&quot;, &quot;extensions&quot;, &quot;networking.k8s.io&quot;]</span><br><span class="line">resources: [&quot;nodes&quot;, &quot;services&quot;, &quot;endpoints&quot;, &quot;namespaces&quot;, &quot;ingresses&quot;, &quot;pods&quot;, &quot;ingressclasses&quot;]</span><br><span class="line">verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]</span><br><span class="line">- apiGroups: [&quot;&quot;, &quot;extensions&quot;, &quot;networking.k8s.io&quot;]</span><br><span class="line">resources: [&quot;configmaps&quot;, &quot;events&quot;, &quot;ingresses/status&quot;, &quot;services/status&quot;]</span><br><span class="line">verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;, &quot;create&quot;, &quot;patch&quot;]</span><br><span class="line">- apiGroups: [&quot;cis.f5.com&quot;]</span><br><span class="line">resources: [&quot;virtualservers&quot;,&quot;virtualservers/status&quot;, &quot;tlsprofiles&quot;, &quot;transportservers&quot;, &quot;ingresslinks&quot;, &quot;externaldnss&quot;]</span><br><span class="line">verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;, &quot;patch&quot;]</span><br><span class="line">- apiGroups: [&quot;fic.f5.com&quot;]</span><br><span class="line">resources: [&quot;f5ipams&quot;, &quot;f5ipams/status&quot;]</span><br><span class="line">verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;, &quot;create&quot;, &quot;patch&quot;, &quot;delete&quot;]</span><br><span class="line">- apiGroups: [&quot;apiextensions.k8s.io&quot;]</span><br><span class="line">resources: [&quot;customresourcedefinitions&quot;]</span><br><span class="line">verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;, &quot;create&quot;, &quot;patch&quot;]</span><br><span class="line">- apiGroups: [&quot;&quot;, &quot;extensions&quot;]</span><br><span class="line">resources: [&quot;secrets&quot;]</span><br><span class="line">verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">name: bigip-ctlr-clusterrole-binding</span><br><span class="line">namespace: kube-system</span><br><span class="line">roleRef:</span><br><span class="line">apiGroup: rbac.authorization.k8s.io</span><br><span class="line">kind: ClusterRole</span><br><span class="line">name: bigip-ctlr-clusterrole</span><br><span class="line">subjects:</span><br><span class="line">- apiGroup: &quot;&quot;</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">name: bigip-ctlr</span><br><span class="line">namespace: kube-system</span><br></pre></td></tr></table></figure></li>
<li><p>应用 YAML 完成 Cluster Role 与 serviceaccounts 的绑定</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f  k8s_rbac.yaml</span><br></pre></td></tr></table></figure></li>
<li><p>在 k8s 集群中, 将 BIG-IP 抽象为一个 Node 对象. 这里的 VtepMAC 地址 <code>00:50:56:86:b4:99</code> 是 BIG-IP 中 flannel_vxlan 的 MAC 地址: <code>tmsh show net tunnels tunnel flannel_vxlan all-properties</code> </p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Node</span><br><span class="line">metadata:</span><br><span class="line">name: bigip</span><br><span class="line">annotations:</span><br><span class="line">    # Provide the MAC address of the BIG-IP VXLAN tunnel</span><br><span class="line">    flannel.alpha.coreos.com/backend-data: &#x27;&#123;&quot;VtepMAC&quot;:&quot;00:50:56:86:b4:99&quot;&#125;&#x27;</span><br><span class="line">    flannel.alpha.coreos.com/backend-type: &quot;vxlan&quot;</span><br><span class="line">    flannel.alpha.coreos.com/kube-subnet-manager: &quot;true&quot;</span><br><span class="line">    # Provide the IP address you assigned as the BIG-IP VTEP</span><br><span class="line">    flannel.alpha.coreos.com/public-ip: 172.16.1.28</span><br><span class="line">spec:</span><br><span class="line"># Define the flannel subnet you want to assign to the BIG-IP device.</span><br><span class="line"># Be sure this subnet does not collide with any other Nodes&#x27; subnets.</span><br><span class="line">podCIDR: 10.244.100.0/24</span><br></pre></td></tr></table></figure></li>
<li><p>ClusterIP 的 cis_clusterip.yaml:</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: k8s-bigip-ctlr-deployment</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: k8s-bigip-ctlr</span><br><span class="line">      labels:</span><br><span class="line">        app: k8s-bigip-ctlr</span><br><span class="line">    spec:</span><br><span class="line">      serviceAccountName: k8s-bigip-ctlr</span><br><span class="line">      containers:</span><br><span class="line">        - name: k8s-bigip-ctlr</span><br><span class="line">          image: &quot;f5networks/k8s-bigip-ctlr:latest&quot;</span><br><span class="line">          imagePullPolicy: IfNotPresent</span><br><span class="line">          env:</span><br><span class="line">            - name: BIGIP_USERNAME</span><br><span class="line">              valueFrom:</span><br><span class="line">                secretKeyRef:</span><br><span class="line">                  name: bigip-login</span><br><span class="line">                  key: username</span><br><span class="line">            - name: BIGIP_PASSWORD</span><br><span class="line">              valueFrom:</span><br><span class="line">                secretKeyRef:</span><br><span class="line">                  name: bigip-login</span><br><span class="line">                  key: password</span><br><span class="line">          command: [&quot;/app/bin/k8s-bigip-ctlr&quot;]</span><br><span class="line">          args: [</span><br><span class="line">            &quot;--bigip-username=$(BIGIP_USERNAME)&quot;,</span><br><span class="line">            &quot;--bigip-password=$(BIGIP_PASSWORD)&quot;,</span><br><span class="line">            &quot;--bigip-url=172.16.1.28&quot;,</span><br><span class="line">            &quot;--bigip-partition=k8s&quot;,</span><br><span class="line">            &quot;--namespace=default&quot;,</span><br><span class="line">            &quot;--pool-member-type=cluster&quot;,</span><br><span class="line">            &quot;--flannel-name=/Common/flannel_vxlan&quot;,</span><br><span class="line">            &quot;--insecure=true&quot;</span><br><span class="line">            ]</span><br></pre></td></tr></table></figure>

<ul>
<li>应用 YAML 完成 CIS 部署  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f  cis_clusterip.yaml</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="部署应用"><a href="#部署应用" class="headerlink" title="部署应用"></a>部署应用</h2><p>完成 CIS 的部署后, 我们就可以尝试在 k8s 集群中部署应用并查看 BIG-IP 上的发布情况. </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: f5-hello-world</span><br><span class="line">  namespace: default</span><br><span class="line">  labels:</span><br><span class="line">    f5type: virtual-server</span><br><span class="line">data:</span><br><span class="line">  schema: &quot;f5schemadb://bigip-virtual-server_v0.1.7.json&quot;</span><br><span class="line">  data: |</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;virtualServer&quot;: &#123;</span><br><span class="line">        &quot;frontend&quot;: &#123;</span><br><span class="line">          &quot;balance&quot;: &quot;round-robin&quot;,</span><br><span class="line">          &quot;mode&quot;: &quot;http&quot;,</span><br><span class="line">          &quot;partition&quot;: &quot;k8s&quot;,</span><br><span class="line">          &quot;virtualAddress&quot;: &#123;</span><br><span class="line">            &quot;bindAddr&quot;: &quot;10.88.0.162&quot;,</span><br><span class="line">            &quot;port&quot;: 80</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;backend&quot;: &#123;</span><br><span class="line">          &quot;serviceName&quot;: &quot;f5-hello-world&quot;,</span><br><span class="line">          &quot;servicePort&quot;: 8080,</span><br><span class="line">          &quot;healthMonitors&quot;: [&#123;</span><br><span class="line">            &quot;interval&quot;: 5,</span><br><span class="line">            &quot;protocol&quot;: &quot;http&quot;,</span><br><span class="line">            &quot;send&quot;: &quot;HEAD / HTTP/1.0\r\n\r\n&quot;,</span><br><span class="line">            &quot;timeout&quot;: 16</span><br><span class="line">          &#125;]</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: f5-hello-world</span><br><span class="line">spec:</span><br><span class="line">  replicas: 100</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        run: f5-hello-world</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: f5-hello-world</span><br><span class="line">        image: &quot;f5devcentral/f5-hello-world:latest&quot;</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8080</span><br><span class="line">          protocol: TCP</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: f5-hello-world</span><br><span class="line">  labels:</span><br><span class="line">    run: f5-hello-world</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - port: 8080</span><br><span class="line">    protocol: TCP</span><br><span class="line">    targetPort: 8080</span><br><span class="line">  type: ClusterIP</span><br><span class="line">  selector:</span><br><span class="line">    run: f5-hello-world</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>在 BIG-IP 上的 pool 以及 virtual server:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># tmsh list ltm pool /k8s/*</span><br><span class="line"></span><br><span class="line">ltm pool /k8s/cfgmap_default_f5-hello-world_f5-hello-world &#123;</span><br><span class="line">    members &#123;</span><br><span class="line">        /k8s/10.244.2.172%0:webcache &#123;</span><br><span class="line">            address 10.244.2.172</span><br><span class="line">            session monitor-enabled</span><br><span class="line">            state up</span><br><span class="line">        &#125;</span><br><span class="line">        /k8s/10.244.2.173%0:webcache &#123;</span><br><span class="line">            address 10.244.2.173</span><br><span class="line">            session monitor-enabled</span><br><span class="line">            state up</span><br><span class="line">        &#125;</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">    metadata &#123;</span><br><span class="line">        user_agent &#123;</span><br><span class="line">            value k8s-bigip-ctlr-1.11.0-n1829-595947561</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    monitor /k8s/cfgmap_default_f5-hello-world_f5-hello-world_0_http</span><br><span class="line">    partition k8s</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># tmsh list ltm virtual /k8s/*</span><br><span class="line">ltm virtual /k8s/default_f5-hello-world &#123;</span><br><span class="line">    creation-time 2021-08-05:05:36:26</span><br><span class="line">    destination /k8s/10.88.0.162%0:http</span><br><span class="line">    ip-protocol tcp</span><br><span class="line">    last-modified-time 2021-08-05:05:36:26</span><br><span class="line">    mask 255.255.255.255</span><br><span class="line">    metadata &#123;</span><br><span class="line">        user_agent &#123;</span><br><span class="line">            value k8s-bigip-ctlr-1.11.0-n1829-595947561</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    partition k8s</span><br><span class="line">    pool /k8s/cfgmap_default_f5-hello-world_f5-hello-world</span><br><span class="line">    profiles &#123;</span><br><span class="line">        http &#123; &#125;</span><br><span class="line">        tcp &#123; &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    source 0.0.0.0/0</span><br><span class="line">    source-address-translation &#123;</span><br><span class="line">        type automap</span><br><span class="line">    &#125;</span><br><span class="line">    translate-address enabled</span><br><span class="line">    translate-port enabled</span><br><span class="line">    vs-index 15</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看到将 BIG-IP 做为 ingress 部署后, BIG-IP 可以自动探测到 k8s 集群中的应用发布变化并在 BIG-IP 上创建相应配置. </p>
<script type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.7.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.7.0/dist/mindmap.min.css">]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>cis</tag>
        <tag>bigip controller</tag>
      </tags>
  </entry>
  <entry>
    <title>eve-ng-connect-external-network</title>
    <url>/cn/eve-ng-connect-external-network/</url>
    <content><![CDATA[<p>我们在<a href="https://www.m4n2.com/cn/eve-ng-install/">这篇文章</a>中简单介绍了 eve-ng 这款软件. 在 eve-ng 的世界中用户可以模拟各种厂商的设备并且方便地搭建实验环境. 然而大家可能会遇到这样一个问题: 如何将 eve-ng 模拟出的 lab 环境与外部网络设备进行连通, 比如在 eve-ng 内运行的 BGP 与 ESXI 上的其他虚拟机建立 BGP 连接?</p>
<span id="more"></span>

<p>我们以在 VMWARE ESXI 上安装的 eve-ng 为例: 在 esxi 上部署 eve-ng 时我们可以为它选择制备的网卡数量. eve-ng 会将第一块网卡称为 management 网络; 而后续的网卡分别对应名称为 Cloud1, Cloud2, Cloud3…以此类推. 而在eve-ng 镜像上的每块网卡都会接入 esxi 的 VM NETWORK上;我们可以将每个 VM NETWORK 想象为一个 VLAN. 所以只要确保 eve-ng 的网卡(CloudX) 与其他虚拟机的网卡接入同一个 VM NETWORK 即可. </p>
<script type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.7.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.7.0/dist/mindmap.min.css">]]></content>
      <categories>
        <category>eve-ng</category>
      </categories>
      <tags>
        <tag>vmware esxi</tag>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu 18.04 安装 gitlab &amp; gitlab-runner</title>
    <url>/cn/gitlab-install/</url>
    <content><![CDATA[<p>本文初步介绍了在 Ubuntu 18.04 上安装 gitlab-ce 以及 gitlab runner 的方法. gitlab-ce 在收到代码的提交后可以触发gitlab runner 执行 CI 命令. gitlab runner 默认使用  <code>gitlab-runner</code> 的账号运行 CI 命令.</p>
<span id="more"></span>

<h2 id="gitlab"><a href="#gitlab" class="headerlink" title="gitlab"></a>gitlab</h2><ul>
<li><p>运行官方提供的脚本添加安装源:</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo curl -sS https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.deb.sh | sudo bash</span><br></pre></td></tr></table></figure>

<p>  安装完成后可以查看可以选择安装的版本:</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apt-cache madison gitlab-ce</span><br></pre></td></tr></table></figure></li>
<li><p>安装 gitlab-ce</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo apt-get install gitlab-ce=13.11.3-ce.0</span><br></pre></td></tr></table></figure></li>
</ul>
<ul>
<li><p>配置 gitlab-ce</p>
<p>  修改 /etc/gitlab/gitlab.rb 中的 external url, 设置为服务器的 IP 地址: external_url ‘<a href="http://192.168.0.1&/#39;">http://192.168.0.1&#39;</a>. 修改完成后应用配置并启动服务</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">gitlab-ctl reconfigure</span><br><span class="line">gitlab-ctl status</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="gitlab-runner"><a href="#gitlab-runner" class="headerlink" title="gitlab runner"></a>gitlab runner</h2><p>安装 gitlab runner 时要尽量选择与 gitlab-ce 相同的版本.</p>
<ul>
<li><p>运行官方提供的脚本添加安装源:</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl -L &quot;https://packages.gitlab.com/install/repositories/runner/gitlab-runner/script.deb.sh&quot; | sudo bash</span><br></pre></td></tr></table></figure>

<p>  安装完成后可以查看可以选择安装的版本:</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apt-cache madison gitlab-runner</span><br></pre></td></tr></table></figure></li>
<li><p>安装 gitlab-runner</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo apt-get install gitlab-runner=13.11.0</span><br></pre></td></tr></table></figure></li>
<li><p>注册 gitlab-runner</p>
<p>  其中 token 在 gitlab 中的 <code>Admin Area &gt; Runner</code> 中可以找到, 并在注册完毕后取消勾选 <code>Lock to current projects</code> 选项以解锁这个gitlab runner.</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">gitlab-runner register</span><br><span class="line"></span><br><span class="line">[root@runner gitlab_ci]# ./gitlab-runner register</span><br><span class="line">Runtime platform                                    arch=amd64 os=linux pid=37194 revision=a8a019e0 version=12.3.0</span><br><span class="line">Running in system-mode.</span><br><span class="line"></span><br><span class="line">Please enter the gitlab-ci coordinator URL (e.g. https://gitlab.com/):</span><br><span class="line">http://10.154.172.173/</span><br><span class="line">Please enter the gitlab-ci token for this runner:</span><br><span class="line">_sAP33YvzuAxX4LnkmP5</span><br><span class="line">Please enter the gitlab-ci description for this runner:</span><br><span class="line">[runner ]: for_test</span><br><span class="line">Please enter the gitlab-ci tags for this runner (comma separated):</span><br><span class="line">for_test</span><br><span class="line">Registering runner... succeeded                     runner=_sAPk3Yv</span><br><span class="line">Please enter the executor: parallels, shell, virtualbox, docker+machine, custom, docker, docker-ssh, ssh, docker-ssh+machine, kubernetes:</span><br><span class="line">shell</span><br><span class="line">Runner registered successfully. Feel free to start it, but if it&#x27;s running already the config should be automatically reloaded!</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
</ul>
<script type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.7.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.7.0/dist/mindmap.min.css">]]></content>
      <categories>
        <category>gitlab</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>gitlab runner</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s 存储初始化之树莓派外接USB硬盘</title>
    <url>/cn/k8s-nfs-as-pv-pvc/</url>
    <content><![CDATA[<p>在<a href="https://www.m4n2.com/cn/k8s-on-rasberry-pi/">之前的文章</a>中我们在 4 台树莓派 model 4b 上搭建了 k8s 集群. 但是这台集群有个小缺陷, 那就是我们为每块树莓派仅仅置备了 64 GB 的 sdcard 做为系统盘以及存储. 这点点空间实在是捉襟见肘, 更不用说后面还要跑一些 BT 下载, 私有云盘之类的应用了. 恰好家里有一块闲置的 3.5 寸 2T 硬盘, 所以就用它来做为 k8s 的存储空间也能算物尽启用吧. 当然如果大家家里有 NAS 设备, 比如群晖, 也完全可以利用其做为 NFS 服务器. 而且因为 NAS 一般都有多硬盘 RAID 方案, 所以数据的存放也更加可靠.</p>
<p>这块硬盘是放置在 ORICO 的硬盘盒中, 硬盘盒自带外接电源而且支持 USB 3.0. 这里遇到一个小插曲, 由于搞错了节点的排序, 所以将硬盘插错了节点… 就这个低级错误白白浪费了几个小时的时间!!! 不过其间也发现了树莓派外接 USB 硬盘的一个坑 - 由于树莓派的 usb 口供电有限, 似乎不少人都遇到无自带电源的 usb 硬盘在树莓派里无法识别. 不过好在我这块是外置电源的硬盘盒, 所以把硬盘插入到正确的节点后树莓派可以正常识别硬盘设备. 所以接下来我们就来研究下如何将这块外接 USB 硬盘变为 k8s 的存储资源.</p>
<span id="more"></span>

<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>我们大致的计划是在 k8s 运行一个 nfs 服务器的 pod, 这个 pod 会让其固定在 node03 上并将外接硬盘 mount 的目录做为 NFS 存储资源发布到集群中供其他 pod 使用.</p>
<p>这块硬盘已经被格式化为 ext4 并 mount 到 /mnt/data 这个目录. 这里就不再赘述关于如何格式化磁盘并挂载的细节, 以下是目前磁盘的挂载路径情况:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ blkid |grep sda</span><br><span class="line">/dev/sda: UUID=&quot;adc0b8fe-2b19-4954-99bc-069e7a3461c2&quot; TYPE=&quot;ext4&quot;</span><br><span class="line"></span><br><span class="line">$ cat /etc/fstab</span><br><span class="line">LABEL=writable  /        ext4   defaults        0 1</span><br><span class="line">LABEL=system-boot       /boot/firmware  vfat    defaults        0       1</span><br><span class="line">UUID=adc0b8fe-2b19-4954-99bc-069e7a3461c2       /mnt/data       ext4    defaults        0       1</span><br><span class="line"></span><br><span class="line">$ df -h | grep data</span><br><span class="line">/dev/sda        1.8T  6.0G  1.7T   1% /mnt/data</span><br></pre></td></tr></table></figure>

<h2 id="准备-PV"><a href="#准备-PV" class="headerlink" title="准备 PV"></a>准备 PV</h2><p>在 k8s 中一个 pod 可以挂载外部存储的方法有很多, 但是比较好的做法是通过 PV 和 PVC 进行挂载. 对这两个概念不熟悉的同学先不用担心, 我们先来了解下为 k8s 中的 pod 提供存储空间需要考虑到的问题. 首先一个 pod 的生命周期非常短暂, 但是 pod 所产生的数据可能需要持久化保存. 比如我们后面会展现的一个例子是在 k8s 中运行一个 BT下载和网盘的服务. 我们希望这个 pod 停止运行后其下载的数据是可以在硬盘上的. 而且运行下载服务的 pod 可能会跑在 k8s 的任意一个节点上, 所以如何让其”活着的时候”有存储用, 而”消亡了后”之前下载的数据又得到了保存就是一个很实际的问题. PV 和 PVC 就能很好的满足我们的需求.</p>
<p>PV (Persistent Volume) 是全局级别的预挂载空间. 它支持与很多类型存储的对接, 比如 NFS, ISCSI, CephFS 等等. 我们可以把 PV 理解为一个在 k8s 世界中提供存储资源的大仓库, 比如你可以有很多个不同类型的 PV; 而 pod 在启动时通过声明 PVC 来获取这些 PV 资源的分配. 我们来看一个实际的例子:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Namespace</span><br><span class="line">metadata:</span><br><span class="line">  name: storage</span><br><span class="line">  labels:</span><br><span class="line">    app: storage</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line">  name: local-pv</span><br><span class="line">  namespace: storage</span><br><span class="line">spec:</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 1500Gi</span><br><span class="line">  accessModes:</span><br><span class="line">  - ReadWriteOnce</span><br><span class="line">  persistentVolumeReclaimPolicy: Retain</span><br><span class="line">  storageClassName: local-storage</span><br><span class="line">  local:</span><br><span class="line">    path: /mnt/data/</span><br><span class="line">  nodeAffinity:</span><br><span class="line">    required:</span><br><span class="line">      nodeSelectorTerms:</span><br><span class="line">      - matchExpressions:</span><br><span class="line">        - key: hdd</span><br><span class="line">          operator: In</span><br><span class="line">          values:</span><br><span class="line">          - enabled</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>为了后期管理的方便我们把这个NFS 服务相关的 pod, service, pv, pvc 都指派到同一个的 namespace - storage 中:</p>
<ul>
<li>这个 pv 资源我们取名为<code>local-pv</code>并分配 1500 GB 的空间</li>
<li>这个 PV 资源的访问模式为<code>ReadWriteOnce</code>: 这个存储卷可以被一个节点以读写方式挂载</li>
<li>persistentVolumeReclaimPolicy 里指定的是回收策略, 这里我们使用 Retain 表示手动回收, 也就是说如果 pod 消亡, 其上的数据得以保留.</li>
<li>storageClassName 指定这个 PV 在 local-storage 这个逻辑组中, 主要是为了后面 PVC 可以直接引用这个 PV</li>
<li>local: 节点上挂载的本地存储设备. 路径是我们移动硬盘挂载在 node03 上的 /mnt/data</li>
<li>nodeAffinity 指定这个 PV 所在的节点. 因为我们的移动硬盘连接在node03上, 所以这个 PV 资源会绑定在 node03. 我们之前为 node03 打上了 <code>hdd=enabled</code> 的 label</li>
</ul>
<p>总结一下: 我们在 node03 创建了一个 pv. 其使用本地目录空间 /mnt/data (实际是外接usb硬盘盒) 做为存储空间; 分配大小为 1500 GB 并且其上的数据只能由管理员手动删除(Retain)!</p>
<h2 id="准备-PVC"><a href="#准备-PVC" class="headerlink" title="准备 PVC"></a>准备 PVC</h2><p>PVC (Persistent Volume Claim) 是对于存储空间的请求声明. 不过这里需要注意的是 PVC 属于名称空间级别. 也就是说 PVC 必须和调用它的 pod 处于同一个 namespace 中. 我们来看下 PVC 的配置:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: local-claim</span><br><span class="line">  namespace: storage</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">  - ReadWriteOnce</span><br><span class="line">  storageClassName: local-storage</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 1500Gi</span><br></pre></td></tr></table></figure>

<ul>
<li>这个 pvc 我们取名为<code>local-claim</code> 并将上面 PV 的所有空间 1500 GB 分配给它.</li>
<li>accessModes 使用和 pv 一致的 <code>ReadWriteOnce</code></li>
<li>storageClassName 中指定与上述 pv 相同的 <code>local-storage</code> 实现与 pv 绑定.</li>
</ul>
<h2 id="准备-nfs-pod"><a href="#准备-nfs-pod" class="headerlink" title="准备 nfs pod"></a>准备 nfs pod</h2><p>我们只需要完成 nfs 服务 pod 资源的准备并在其调用上面的 pvc 即可. nfs 服务器的 deployment 如下:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-server</span><br><span class="line">  namespace: storage</span><br><span class="line">  labels:</span><br><span class="line">    app: nfs-server</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nfs-server</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nfs-server</span><br><span class="line">        name: nfs-server</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nfs-server</span><br><span class="line">        image: itsthenetwork/nfs-server-alpine:latest-arm</span><br><span class="line">        env:</span><br><span class="line">          - name: SHARED_DIRECTORY</span><br><span class="line">            value: /exports</span><br><span class="line">        ports:</span><br><span class="line">          - name: nfs</span><br><span class="line">            containerPort: 2049</span><br><span class="line">          - name: mountd</span><br><span class="line">            containerPort: 20048</span><br><span class="line">          - name: rpcbind</span><br><span class="line">            containerPort: 111</span><br><span class="line">        securityContext:</span><br><span class="line">          privileged: true</span><br><span class="line">        volumeMounts:</span><br><span class="line">          - mountPath: /exports</span><br><span class="line">            name: mypvc</span><br><span class="line">      volumes:</span><br><span class="line">        - name: mypvc</span><br><span class="line">          persistentVolumeClaim:</span><br><span class="line">            claimName: local-claim</span><br><span class="line">      nodeSelector:</span><br><span class="line">        hdd: enabled</span><br></pre></td></tr></table></figure>

<ul>
<li>我们 nfs 应用 pod 名字为nfs-server 并打上 <code>app: nfs-server</code> 的标签. 它运行在和 pv, pvc 相同的 namespace 中</li>
<li>将带有<code>app: nfs-server</code>标签的 pod 绑定到这个 deployment 资源里</li>
<li>env 中传入环境变量指定 nfs 服务器将存储挂载到自己的 /exports 目录下</li>
<li>ports 中暴露服务端口</li>
<li>securityContext 比较重要. 这个docker image 需要这样设置才能在 k8s 中工作</li>
<li>volumeMounts: 将 pod 内的 /exports 目录取名 mypvc 做为挂载点</li>
<li>volumes: 把 local-claim 这个 pvc 挂载到 <code>mypvc</code> 这个 pod 中的挂载点上</li>
<li>nodeSelector再次指定这个pod 需要运行在 node03 上</li>
</ul>
<h2 id="Service-准备"><a href="#Service-准备" class="headerlink" title="Service 准备"></a>Service 准备</h2><p>最后我们准备 service 资源将这个 nfs 应用发布出去. 我们为这个 service 分配了固定 IP: 10.96.0.100. 同时这个service 资源绑定到有<code>app: nfs-server</code>的pod 上. </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-server</span><br><span class="line">  namespace: storage</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">    - name: nfs</span><br><span class="line">      port: 2049</span><br><span class="line">    - name: mountd</span><br><span class="line">      port: 20048</span><br><span class="line">    - name: rpcbind</span><br><span class="line">      port: 111</span><br><span class="line">  clusterIP: 10.96.0.100 this.</span><br><span class="line">  selector:</span><br><span class="line">    app: nfs-server</span><br></pre></td></tr></table></figure>

<p>这样我们就完成了这个 nfs pod 的配置. 如果要在 k8s 上运行可以将上述配置放入同一个 yaml 文件中然后运行:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f nfs.yml</span><br></pre></td></tr></table></figure>

<p>另外请确保所有的 k8s 节点上都安装有 nfs 的client 程序. 因为后续的 pod 其实都是通过 mount 来调用这个 nfs 共享出的存储资源. 安装 nfs 客户端可以在各台node 主机上运行:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo apt update &amp;&amp;&amp; sudo apt-get install nfs-common -y</span><br></pre></td></tr></table></figure>

<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>为 k8s 添加了存储资源后, 我们后续就可以方便的为 pod 分配空间并且数据会存放在 nfs 上进行持久存储. 我们下一步就是搭建一个家用的文件下载服务器以及私有云盘. 敬请期待把!</p>
<script type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.7.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.7.0/dist/mindmap.min.css">]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>raspberry pi</tag>
      </tags>
  </entry>
  <entry>
    <title>在树莓派上点亮你的 Kubernetes 集群</title>
    <url>/cn/k8s-on-rasberry-pi/</url>
    <content><![CDATA[<p>提到 raspberry pi 相信大家都不陌生, 简单来说它就是一台名片大小基于 ARM 架构的计算机. 经过几代的发展 raspberry pi 的硬件配置有了长足的进步. 在目前最新的 4b 版本上, raspberry pi 可以提供 4 核, 8GB RAM 的配置. 这也使得它的可玩性大大提高. 最近恰好在学习 Kubernetes, 所以就想到用 raspberry pi 来组一台 Kubernetes 集群. 对于这台集群我希望它拥有低功耗, 安静, 但最主要是可以稳定地运行多个服务. 这篇文章是这个系列的第一篇, 先来介绍如何在一台由4块 raspberry pi 4b 组成的硬件集群上搭建 kubernetes cluster.</p>
<span id="more"></span>

<h2 id="硬件准备"><a href="#硬件准备" class="headerlink" title="硬件准备"></a>硬件准备</h2><p>我这次选择了 4 台 raspberry pi 4b 8GB 版本. 其中一台 master, 另外三台 node 节点. 树莓派默认需要一个 5V 3A 的 type c 电源以及一个千兆有线网口, 这样如果要部署一个 4 节点的集群就要准备 8 根线 ( 4 根电源线 + 4 根网线). 考虑到布线的简洁清爽, 我为树莓派加装了一个 POE 模块, 有了 POE 模块就可以用一根网线同时提供网络和供电功能, 这样只需要 4 根网线以及一台 POE 交换机/路由器即可解决所有的供电和网络需求. 另外每台树莓派配有一块 64GB 的 sdcard 用来安装系统. </p>
<h2 id="软件准备"><a href="#软件准备" class="headerlink" title="软件准备"></a>软件准备</h2><p>操作系统方面我们选择目前最新的 LTS 版本, Ubuntu 20.04 版. 我们使用 usbimager  将ubuntu 镜像刷入 sdcard. Kubernetes 安装最新的稳定版本 1.22.2. 需要用到的软件下载地址如下:</p>
<ul>
<li><a href="https://ubuntu.com/download/raspberry-pi">Ubuntu 20.04 64 bit for raspberry pi</a></li>
<li><a href="https://gitlab.com/bztsrc/usbimager">usbimage</a></li>
</ul>
<h2 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h2><h3 id="初步准备"><a href="#初步准备" class="headerlink" title="初步准备"></a>初步准备</h3><ol>
<li><p>usbimager 使用非常简单, 在界面中选择下载的20.04镜像并指定目标 sdcard 设备进行写入. 如果 sdcard 上存在其他系统或者分区, 建议可以将这些分区先删除后再进行系统刷入.</p>
</li>
<li><p>完成 4 台设备的系统刷入后, 将 sdcard 插入树莓派并连接到POE进行初步测试. 树莓派默认通过 DHCP 获取 IP 地址. 因此如果顺利的话, 可以在路由器的管理界面中看到树莓派分配的 IP 地址. 为了方便今后的使用, 我们可以在此时绑定 MAC 与 IP 地址的分配. 这样 4 台树莓派都获得了固定的 IP 地址. 在我的环境中 4 台树莓派的地址分配如下:</p>
</li>
</ol>
<ul>
<li>master01: 172.16.0.1</li>
<li>node01: 172.16.0.11</li>
<li>node02: 172.16.0.12</li>
<li>node03: 172.16.0.13</li>
</ul>
<h3 id="系统准备"><a href="#系统准备" class="headerlink" title="系统准备"></a>系统准备</h3><p>在<code>所有的树莓派</code>上完成以下准备工作:</p>
<ol start="0">
<li><p>更新系统: <code>sudo apt update &amp;&amp; sudo apt dist-upgrade &amp;&amp; apt-get install -y apt-transport-https</code></p>
</li>
<li><p>创建自定义用户: <code>sudo adduser manfred</code></p>
</li>
<li><p>给予该用户 sudo 权限: <code>sudo usermod -aG sudo manfred</code></p>
</li>
<li><p>设置主机名(注意每个node 的名字不同): <code>echo master01 | sudo tee /etc/hostname</code></p>
</li>
<li><p>为集群内所有节点添加 hosts 文件映射:</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># cat /etc/hosts</span><br><span class="line">127.0.0.1       localhost</span><br><span class="line">172.16.0.1      master01</span><br><span class="line">172.16.0.11     node01</span><br><span class="line">172.16.0.12     node02</span><br><span class="line">172.16.0.13     node03</span><br></pre></td></tr></table></figure></li>
<li><p>编辑 /boot/firmware/cmdline.txt 并在行末添加:</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cgroup_enable=cpuset cgroup_enable=memory cgroup_memory=1 swapaccount=1</span><br></pre></td></tr></table></figure></li>
<li><p>重启: <code>sudo reboot</code></p>
</li>
<li><p>安装 docker: <code>curl -sSL get.docker.com | sudo sh</code></p>
</li>
<li><p>将自己的账号加入 docker 组: <code>sudo usermod -aG docker manfred</code></p>
</li>
<li><p>编辑 docker 配置文件并指定中文源: <code>sudo vi /etc/docker/daemon.json</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],</span><br><span class="line">  &quot;log-driver&quot;: &quot;json-file&quot;,</span><br><span class="line">  &quot;log-opts&quot;: &#123;</span><br><span class="line">    &quot;max-size&quot;: &quot;100m&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;storage-driver&quot;: &quot;overlay2&quot;,</span><br><span class="line">  &quot;registry-mirrors&quot;: [&quot;https://docker.mirrors.ustc.edu.cn&quot;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>应用上述配置: <code>sudo systemctl daemon-reload &amp;&amp; sudo systemctl restart docker</code> </p>
</li>
<li><p>开启路由: <code>sudo sed -i &#39;s/#net.ipv4.ip_forward=1/net.ipv4.ip_forward=1/g&#39; /etc/sysctl.conf</code></p>
</li>
<li><p>重启系统: <code>sudo reboot</code></p>
</li>
<li><p>重启后确定 docker 工作正常: docker run hello-world </p>
</li>
</ol>
<p>完成以上步骤后, 下面开始进入 kubernetes 的安装过程.</p>
<h3 id="Kubernetes-安装准备"><a href="#Kubernetes-安装准备" class="headerlink" title="Kubernetes 安装准备"></a>Kubernetes 安装准备</h3><p>在所有树莓派上执行以下步骤:</p>
<ol>
<li>为Kubernetes 设置国内源:<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add - </span><br><span class="line">cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list</span><br><span class="line">deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main</span><br><span class="line">EOF  </span><br><span class="line">sudo apt-get update</span><br></pre></td></tr></table></figure></li>
<li>由于我们安装的版本 1.22.2 比较新, 所以可以手动下载这些镜像并用 tag 命令进行修正:<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo docker pull coredns/coredns:1.8.4</span><br><span class="line">sudo docker tag coredns/coredns:1.8.4 registry.aliyuncs.com/google_containers/coredns:v1.8.4</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sudo docker pull cjk2atmb/kube-controller-manager:v1.22.2</span><br><span class="line">sudo docker tag cjk2atmb/kube-controller-manager:v1.22.2 registry.aliyuncs.com/google_containers/kube-controller-manager:v1.22.2</span><br></pre></td></tr></table></figure></li>
<li>在上述准备工作完成后我们就可以开始安装: <code>sudo apt-get install -y kubelet kubeadm kubectl</code></li>
</ol>
<h3 id="master-节点"><a href="#master-节点" class="headerlink" title="master 节点"></a>master 节点</h3><ol>
<li><p>初始化 Kubernetes master 节点: </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo kubeadm init --image-repository=registry.aliyuncs.com/google_containers --pod-network-cidr=10.244.0.0/16</span><br></pre></td></tr></table></figure></li>
<li><p>如果一切正常的话在上述命令运行完后会看到如下提示. 将下面的 kubeadm join 命令保存好以备后续添加 node 节点时使用:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Your Kubernetes control-plane has initialized successfully!</span><br><span class="line"></span><br><span class="line">To start using your cluster, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">  mkdir -p $HOME/.kube</span><br><span class="line">  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">  sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br><span class="line"></span><br><span class="line">Alternatively, if you are the root user, you can run:</span><br><span class="line"></span><br><span class="line">  export KUBECONFIG=/etc/kubernetes/admin.conf</span><br><span class="line"></span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:</span><br><span class="line">  https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br><span class="line"></span><br><span class="line">Then you can join any number of worker nodes by running the following on each as root:</span><br><span class="line"></span><br><span class="line">sudo kubeadm join 172.16.0.1:6443 --token xxxxxxx \</span><br><span class="line">        --discovery-token-ca-cert-hash sha256:xxxxx</span><br></pre></td></tr></table></figure></li>
<li><p>运行以下命令初始化 kubectl:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir -p $HOME/.kube</span><br><span class="line">sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br><span class="line">echo &#x27;source &lt;(kubectl completion bash)&#x27; &gt;&gt; ~/.bashrc</span><br></pre></td></tr></table></figure></li>
<li><p>安装 flannel 网络组件:<br><code>kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</code></p>
</li>
<li><p>最后确保所有镜像均可正常运行:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">NAMESPACE     NAME                               READY   STATUS    RESTARTS       AGE</span><br><span class="line">default       demoapp-79b7d5f68b-bp9z2           1/1     Running   0              30h</span><br><span class="line">kube-system   coredns-7f6cbbb7b8-kwsng           1/1     Running   1 (41h ago)    42h</span><br><span class="line">kube-system   coredns-7f6cbbb7b8-qbmdn           1/1     Running   1 (41h ago)    42h</span><br><span class="line">kube-system   etcd-master01                      1/1     Running   5 (41h ago)    42h</span><br><span class="line">kube-system   kube-apiserver-master01            1/1     Running   5 (41h ago)    42h</span><br><span class="line">kube-system   kube-controller-manager-master01   1/1     Running   1 (41h ago)    42h</span><br><span class="line">kube-system   kube-flannel-ds-kghwp              1/1     Running   1 (41h ago)    41h</span><br><span class="line">kube-system   kube-flannel-ds-rcjfq              1/1     Running   1 (41h ago)    42h</span><br><span class="line">kube-system   kube-proxy-6mksg                   1/1     Running   1 (41h ago)    42h</span><br><span class="line">kube-system   kube-proxy-w95hs                   1/1     Running   1 (41h ago)    41h</span><br><span class="line">kube-system   kube-scheduler-master01            1/1     Running   5 (41h ago)    42h</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="node-节点"><a href="#node-节点" class="headerlink" title="node 节点"></a>node 节点</h3><ol>
<li><p>复制之前的 join 命令:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo kubeadm join 172.16.0.1:6443 --token xxxxxxx --discovery-token-ca-cert-hash sha256:xxxxx </span><br></pre></td></tr></table></figure>
<p>如果 token 丢失或者过期可以重新生成: <code>kubeadm token create --print-join-command</code> </p>
</li>
<li><p>安装完成后用 kubectl get nodes 查看安装情况. 如果 node 一直显示<code>Not Ready</code>, 可以尝试重启 node. 如果 node 没有 role, 可以自己给node 打role 标签: </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl label node node01 node-role.kubernetes.io/worker=worker</span><br></pre></td></tr></table></figure></li>
</ol>
<p>这样基本完成了Kubernetes 集群的安装. </p>
<script type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.7.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.7.0/dist/mindmap.min.css">]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>raspberry pi</tag>
      </tags>
  </entry>
  <entry>
    <title>nmap</title>
    <url>/cn/nmap/</url>
    <content><![CDATA[<p>nmap 可以用来快速发现网络中的主机 IP 以及监听的端口. 同时 nmap 还可以猜测对端操作系统的类型, 获取服务器的证书, 到达目标主机的 traceroute 信息等. 结合 “–script” 参数还可以调用扫描脚本对特定服务进行更加精细化的扫描. </p>
<span id="more"></span>

<h2 id="检测目标主机监听端口"><a href="#检测目标主机监听端口" class="headerlink" title="检测目标主机监听端口"></a>检测目标主机监听端口</h2><p>nmap 可以检测某个 IP, 多个 IP 或者一个网段内 IP 地址的监听端口(默认tcp):</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nmap 10.0.0.1</span><br><span class="line">nmap 10.0.0.1 10.0.0.20 10.0.0.60</span><br><span class="line">nmap 10.0.0.0/24</span><br><span class="line">nmap 10.0.0.*</span><br><span class="line"></span><br><span class="line"># 扫描文件中列出的主机列表</span><br><span class="line">nmap -iL /tmp/test.txt</span><br><span class="line"></span><br><span class="line">cat /tmp/test.txt</span><br><span class="line">server1.cyberciti.biz</span><br><span class="line">192.168.1.0/24</span><br><span class="line">192.168.1.1/24</span><br><span class="line">10.1.2.3</span><br><span class="line">localhost</span><br><span class="line"></span><br><span class="line"># 排除10.0.0.60 和 10.0.0.80</span><br><span class="line">nmap 10.0.0.0/24 --exclude 10.0.0.60,10.0.0.80 </span><br><span class="line"></span><br><span class="line"># 排除exclude.txt文件内主机</span><br><span class="line">nmap 10.0.0.0/24 -iL /tmp/scanlist.txt --excludefile /tmp/exclude.txt</span><br><span class="line"></span><br><span class="line"># 扫描 1~20 主机</span><br><span class="line">nmap 10.0.0.1-20 </span><br></pre></td></tr></table></figure>


<h2 id="猜测操作系统版本"><a href="#猜测操作系统版本" class="headerlink" title="猜测操作系统版本"></a>猜测操作系统版本</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 对主机进行全面检查: OS detection, version detection, script scanning, and traceroute</span><br><span class="line">nmap -v -A 192.168.1.254</span><br><span class="line"></span><br><span class="line"># 对主机只进行操作系统猜测, 当Nmap无法确定所检测的操作系统时，会尽可能地提供最相近的匹配，Nmap默认 进行这种匹配，使用上述任一个选项使得Nmap的推测更加有效</span><br><span class="line">nmap -v -O --osscan-guess 10.0.0.1</span><br><span class="line">nmap -v -O --osscan-guess --fuzzy 10.0.0.1</span><br></pre></td></tr></table></figure>

<h2 id="检测主机前是否有-firewall"><a href="#检测主机前是否有-firewall" class="headerlink" title="检测主机前是否有 firewall"></a>检测主机前是否有 firewall</h2><p>用来检测主机前是否有防火墙. -sA 也被称为 ACK Scan. 注意其目的不是绕过防火墙, 而是检测是否有防火墙存在. 当被检测主机接受到 ACK 时, 无论端口是否处于 open 状态都会直接返回 RST. </p>
<ul>
<li>如果 nmap 收到 RST 包, 则表示数据包并没有经过防火墙; </li>
<li>如果收到 ICMP unreachable error (type 3, code 1, 2, 3, 9, 10, or 13), 或者没有任何回应数据包则表示 ACK 可能被防火墙丢弃.</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># nmap -sA 10.154.172.172</span><br><span class="line">Starting Nmap 7.70 ( https://nmap.org ) at 2021-09-02 19:31 CST</span><br><span class="line">Nmap scan report for 10.154.172.172</span><br><span class="line">Host is up (0.00070s latency).</span><br><span class="line">All 1000 scanned ports on 10.154.172.172 are filtered</span><br><span class="line">MAC Address: 00:50:56:86:47:46 (VMware)</span><br></pre></td></tr></table></figure>


<h2 id="扫描-IPV6-地址"><a href="#扫描-IPV6-地址" class="headerlink" title="扫描 IPV6 地址"></a>扫描 IPV6 地址</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nmap -6 IPv6-Address-Here</span><br><span class="line">nmap -6 server1.cyberciti.biz</span><br><span class="line">nmap -6 2607:f0d0:1002:51::4</span><br><span class="line">nmap -v A -6 2607:f0d0:1002:51::4</span><br></pre></td></tr></table></figure>

<h2 id="快速扫描子网内-up-主机"><a href="#快速扫描子网内-up-主机" class="headerlink" title="快速扫描子网内 up 主机"></a>快速扫描子网内 up 主机</h2><p>在特权账号下 nmap 使用 ICMP echo request, TCP SYN to port 443, TCP ACK to port 80, and an ICMP timestamp 来探测网络中的主机, 而对于非特权用户, nmap 只会使用 80 或者 443 来进行扫描. </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># nmap -sP 10.88.0.0/24</span><br><span class="line">Starting Nmap 7.70 ( https://nmap.org ) at 2021-09-02 19:00 CST</span><br><span class="line">Nmap scan report for 10.88.0.105</span><br><span class="line">Host is up (0.00089s latency).</span><br><span class="line">MAC Address: 00:50:56:86:D3:88 (VMware)</span><br><span class="line">Nmap scan report for 10.88.0.115</span><br><span class="line">Host is up (0.0010s latency).</span><br><span class="line">MAC Address: 00:50:56:86:5C:19 (VMware)</span><br><span class="line">Nmap scan report for 10.88.0.116</span><br><span class="line">Host is up (0.00062s latency).</span><br><span class="line">MAC Address: 00:50:56:86:3C:3A (VMware)</span><br><span class="line">Nmap scan report for 10.88.0.129</span><br><span class="line">Host is up (0.00051s latency).</span><br><span class="line">MAC Address: 00:50:56:86:90:5D (VMware)</span><br><span class="line">Nmap scan report for 10.88.0.153</span><br><span class="line">Host is up (0.00083s latency).</span><br><span class="line">MAC Address: 00:50:56:86:5A:02 (VMware)</span><br><span class="line">Nmap scan report for 10.88.0.162</span><br><span class="line">Host is up (0.0042s latency).</span><br><span class="line">MAC Address: 00:50:56:86:D3:88 (VMware)</span><br><span class="line">Nmap scan report for 10.88.0.200</span><br><span class="line">Host is up.</span><br><span class="line">Nmap done: 256 IP addresses (7 hosts up) scanned in 3.98 seconds</span><br></pre></td></tr></table></figure>

<h2 id="显示-nmap-发出的包"><a href="#显示-nmap-发出的包" class="headerlink" title="显示 nmap 发出的包"></a>显示 nmap 发出的包</h2><p>另外如果要观察 nmap 发送了哪些扫描包, 可以加上 <code>--packet-trace</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nmap -sP --packet-trace --send-ip 10.88.0.0/24</span><br></pre></td></tr></table></figure>

<h2 id="列出-nmap-扫描结果的理由"><a href="#列出-nmap-扫描结果的理由" class="headerlink" title="列出 nmap 扫描结果的理由"></a>列出 nmap 扫描结果的理由</h2><p><code>--reason</code> 可以写明 nmap 得出扫描结果的依据, 比如下面的例子中 nmap 是收到了 ICMP echo-reply 来判定 10.88.0.105 处于 up 状态</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nmap -sP --packet-trace --send-ip --reason 10.88.0.0/24</span><br><span class="line"></span><br><span class="line">Nmap scan report for 10.88.0.105</span><br><span class="line">Host is up, received echo-reply ttl 255 (0.00041s latency).</span><br><span class="line">MAC Address: 00:50:56:86:D3:88 (VMware)</span><br></pre></td></tr></table></figure>

<h2 id="检测-TLS-服务端支持的-cipher-list"><a href="#检测-TLS-服务端支持的-cipher-list" class="headerlink" title="检测 TLS 服务端支持的 cipher list"></a>检测 TLS 服务端支持的 cipher list</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ nmap --script ssl-enum-ciphers -p 443 phishme.com</span><br><span class="line"></span><br><span class="line">Starting Nmap 6.25 ( http://nmap.org ) at 2014-10-21 16:56 PDT</span><br><span class="line">Nmap scan report <span class="keyword">for</span> phishme.com (198.90.20.111)</span><br><span class="line">Host is up (0.011s latency).</span><br><span class="line">PORT STATE SERVICE</span><br><span class="line">443/tcp open https</span><br><span class="line">| ssl-enum-ciphers:</span><br><span class="line">| SSLv3:</span><br><span class="line">| ciphers:</span><br><span class="line">| TLS_DHE_RSA_WITH_3DES_EDE_CBC_SHA - strong</span><br><span class="line">| TLS_DHE_RSA_WITH_AES_128_CBC_SHA - strong</span><br><span class="line">| TLS_DHE_RSA_WITH_AES_256_CBC_SHA - strong</span><br><span class="line">| TLS_DHE_RSA_WITH_CAMELLIA_128_CBC_SHA - strong</span><br><span class="line">| TLS_DHE_RSA_WITH_CAMELLIA_256_CBC_SHA - strong</span><br><span class="line">| TLS_DHE_RSA_WITH_SEED_CBC_SHA - strong</span><br><span class="line">| TLS_RSA_WITH_3DES_EDE_CBC_SHA - strong</span><br><span class="line">| TLS_RSA_WITH_AES_128_CBC_SHA - strong</span><br><span class="line">| TLS_RSA_WITH_AES_256_CBC_SHA - strong</span><br><span class="line">| TLS_RSA_WITH_CAMELLIA_128_CBC_SHA - strong</span><br><span class="line">| TLS_RSA_WITH_CAMELLIA_256_CBC_SHA - strong</span><br><span class="line">| TLS_RSA_WITH_RC4_128_SHA - strong</span><br><span class="line">| TLS_RSA_WITH_SEED_CBC_SHA - strong</span><br><span class="line">| compressors:</span><br><span class="line">| NULL</span><br><span class="line">| TLSv1.0:</span><br><span class="line">| ciphers:</span><br><span class="line">| TLS_DHE_RSA_WITH_3DES_EDE_CBC_SHA - strong</span><br><span class="line">| TLS_DHE_RSA_WITH_AES_128_CBC_SHA - strong</span><br><span class="line">| TLS_DHE_RSA_WITH_AES_256_CBC_SHA - strong</span><br><span class="line">| TLS_DHE_RSA_WITH_CAMELLIA_128_CBC_SHA - strong</span><br><span class="line">| TLS_DHE_RSA_WITH_CAMELLIA_256_CBC_SHA - strong</span><br><span class="line">| TLS_DHE_RSA_WITH_SEED_CBC_SHA - strong</span><br><span class="line">| TLS_RSA_WITH_3DES_EDE_CBC_SHA - strong</span><br><span class="line">| TLS_RSA_WITH_AES_128_CBC_SHA - strong</span><br><span class="line">| TLS_RSA_WITH_AES_256_CBC_SHA - strong</span><br><span class="line">| TLS_RSA_WITH_CAMELLIA_128_CBC_SHA - strong</span><br><span class="line">| TLS_RSA_WITH_CAMELLIA_256_CBC_SHA - strong</span><br><span class="line">| TLS_RSA_WITH_RC4_128_SHA - strong</span><br><span class="line">| TLS_RSA_WITH_SEED_CBC_SHA - strong</span><br><span class="line">| compressors:</span><br><span class="line">| NULL</span><br><span class="line">| TLSv1.1:</span><br><span class="line">| ciphers:</span><br><span class="line">| TLS_DHE_RSA_WITH_3DES_EDE_CBC_SHA - strong</span><br><span class="line">| TLS_DHE_RSA_WITH_AES_128_CBC_SHA - strong</span><br><span class="line">| TLS_DHE_RSA_WITH_AES_256_CBC_SHA - strong</span><br><span class="line">| TLS_DHE_RSA_WITH_CAMELLIA_128_CBC_SHA - strong</span><br><span class="line">| TLS_DHE_RSA_WITH_CAMELLIA_256_CBC_SHA - strong</span><br><span class="line">| TLS_DHE_RSA_WITH_SEED_CBC_SHA - strong</span><br><span class="line">| TLS_RSA_WITH_3DES_EDE_CBC_SHA - strong</span><br><span class="line">| TLS_RSA_WITH_AES_128_CBC_SHA - strong</span><br><span class="line">| TLS_RSA_WITH_AES_256_CBC_SHA - strong</span><br><span class="line">| TLS_RSA_WITH_CAMELLIA_128_CBC_SHA - strong</span><br><span class="line">| TLS_RSA_WITH_CAMELLIA_256_CBC_SHA - strong</span><br><span class="line">| TLS_RSA_WITH_RC4_128_SHA - strong</span><br><span class="line">| TLS_RSA_WITH_SEED_CBC_SHA - strong</span><br><span class="line">| compressors:</span><br><span class="line">| NULL</span><br><span class="line">| TLSv1.2:</span><br><span class="line">| ciphers:</span><br><span class="line">| TLS_DHE_RSA_WITH_3DES_EDE_CBC_SHA - strong</span><br><span class="line">| TLS_DHE_RSA_WITH_AES_128_CBC_SHA - strong</span><br><span class="line">| TLS_DHE_RSA_WITH_AES_128_CBC_SHA256 - strong</span><br><span class="line">| TLS_DHE_RSA_WITH_AES_128_GCM_SHA256 - strong</span><br><span class="line">| TLS_DHE_RSA_WITH_AES_256_CBC_SHA - strong</span><br><span class="line">| TLS_DHE_RSA_WITH_AES_256_CBC_SHA256 - strong</span><br><span class="line">| TLS_DHE_RSA_WITH_AES_256_GCM_SHA384 - strong</span><br><span class="line">| TLS_DHE_RSA_WITH_CAMELLIA_128_CBC_SHA - strong</span><br><span class="line">| TLS_DHE_RSA_WITH_CAMELLIA_256_CBC_SHA - strong</span><br><span class="line">| TLS_DHE_RSA_WITH_SEED_CBC_SHA - strong</span><br><span class="line">| TLS_RSA_WITH_3DES_EDE_CBC_SHA - strong</span><br><span class="line">| TLS_RSA_WITH_AES_128_CBC_SHA - strong</span><br><span class="line">| TLS_RSA_WITH_AES_128_CBC_SHA256 - strong</span><br><span class="line">| TLS_RSA_WITH_AES_128_GCM_SHA256 - strong</span><br><span class="line">| TLS_RSA_WITH_AES_256_CBC_SHA - strong</span><br><span class="line">| TLS_RSA_WITH_AES_256_CBC_SHA256 - strong</span><br><span class="line">| TLS_RSA_WITH_AES_256_GCM_SHA384 - strong</span><br><span class="line">| TLS_RSA_WITH_CAMELLIA_128_CBC_SHA - strong</span><br><span class="line">| TLS_RSA_WITH_CAMELLIA_256_CBC_SHA - strong</span><br><span class="line">| TLS_RSA_WITH_RC4_128_SHA - strong</span><br><span class="line">| TLS_RSA_WITH_SEED_CBC_SHA - strong</span><br><span class="line">| compressors:</span><br><span class="line">| NULL</span><br><span class="line">|_ least strength: strong</span><br><span class="line"></span><br><span class="line">Nmap <span class="keyword">done</span>: 1 IP address (1 host up) scanned <span class="keyword">in</span> 3.46 seconds</span><br></pre></td></tr></table></figure>

<h2 id="检测-SSH-服务端支持的Cipher"><a href="#检测-SSH-服务端支持的Cipher" class="headerlink" title="检测 SSH 服务端支持的Cipher"></a>检测 SSH 服务端支持的Cipher</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># nmap --script ssh2-enum-algos -sV -p 22 10.155.216.4</span><br><span class="line">Starting Nmap 7.70 ( https://nmap.org ) at 2021-09-01 20:58 CST</span><br><span class="line">Nmap scan report for 10.155.216.4</span><br><span class="line">Host is up (0.0011s latency).</span><br><span class="line"></span><br><span class="line">PORT   STATE SERVICE VERSION</span><br><span class="line">22/tcp open  ssh     OpenSSH 7.4 (protocol 2.0)</span><br><span class="line">| ssh2-enum-algos:</span><br><span class="line">|   kex_algorithms: (4)</span><br><span class="line">|       ecdh-sha2-nistp256</span><br><span class="line">|       ecdh-sha2-nistp384</span><br><span class="line">|       diffie-hellman-group14-sha1</span><br><span class="line">|       diffie-hellman-group-exchange-sha1</span><br><span class="line">|   server_host_key_algorithms: (5)</span><br><span class="line">|       ssh-rsa</span><br><span class="line">|       rsa-sha2-512</span><br><span class="line">|       rsa-sha2-256</span><br><span class="line">|       ecdsa-sha2-nistp256</span><br><span class="line">|       ssh-ed25519</span><br><span class="line">|   encryption_algorithms: (8)</span><br><span class="line">|       aes128-gcm@openssh.com</span><br><span class="line">|       aes256-gcm@openssh.com</span><br><span class="line">|       aes128-cbc</span><br><span class="line">|       aes256-cbc</span><br><span class="line">|       aes128-ctr</span><br><span class="line">|       aes192-ctr</span><br><span class="line">|       aes256-ctr</span><br><span class="line">|       aes192-cbc</span><br><span class="line">|   mac_algorithms: (1)</span><br><span class="line">|       hmac-sha1</span><br><span class="line">|   compression_algorithms: (2)</span><br><span class="line">|       none</span><br><span class="line">|_      zlib@openssh.com</span><br><span class="line"></span><br></pre></td></tr></table></figure><script type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.7.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.7.0/dist/mindmap.min.css">]]></content>
      <categories>
        <category>oscp</category>
      </categories>
      <tags>
        <tag>nmap</tag>
        <tag>scanner</tag>
      </tags>
  </entry>
  <entry>
    <title>mailu 搭建邮件服务器</title>
    <url>/cn/mailu/</url>
    <content><![CDATA[<p>mailu 是一款非常不错的邮件服务器解决方案。在 lab 中可以利用 docker 和 docker compose 快速的提供邮件服务。</p>
<span id="more"></span>


<ol>
<li>为 linux 安装 docker 以及 docker compose </li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apt install docker docker-compose</span><br></pre></td></tr></table></figure>

<ol start="2">
<li><p>切换到 root 并在根目录下建立 /mailu</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir /mailu;cd /mailu</span><br></pre></td></tr></table></figure></li>
<li><p>在网站 <a href="https://setup.mailu.io/">https://setup.mailu.io/</a> 上选择稳定版，这里我们选择1.9 然后在网页中提供配置参数. 提供配置参数后网站会提供下载两个文件： docker-compose.yml 和 mailu.env</p>
</li>
<li><p>启动 mailu</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd /mailu</span><br><span class="line">docker-compose -p mailu up -d</span><br></pre></td></tr></table></figure></li>
</ol>
<p>另外关闭 mailu</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker-compose -p mailu down</span><br></pre></td></tr></table></figure>

<ol start="5">
<li><p>初始化管理员密码。注意替换下面的 PASSWORD</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker-compose -p mailu exec admin flask mailu admin admin m4n2.com PASSWORD</span><br></pre></td></tr></table></figure></li>
<li><p>mailu 可以提供下列服务。普通邮件用户可以登录Web Mail 后再进行创建</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Webmail Portal: https://&lt;IP&gt;/Webmail</span><br><span class="line">管理员 portal: https://&lt;IP&gt;/admin</span><br><span class="line">用户名：admin</span><br><span class="line">密码： 参考步骤 5</span><br></pre></td></tr></table></figure></li>
</ol>
<script type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.7.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.7.0/dist/mindmap.min.css">]]></content>
      <categories>
        <category>smtp</category>
      </categories>
      <tags>
        <tag>email server</tag>
      </tags>
  </entry>
  <entry>
    <title>eve-ng 安装初始化</title>
    <url>/cn/eve-ng-install/</url>
    <content><![CDATA[<p>一次偶然的机会接触到了 eve-ng, 一款模拟各种 IT 环境的软件. 用户可以通过 web 页面来创建各种 lab;在 lab 中可以拖拽需要的设备搭建各种拓扑. 而且 eve-ng 官方网站的资源非常清晰, 上手也很容易. 这篇文章对快速安装以及相关资源做一个索引.</p>
<span id="more"></span>

<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>eve-ng 分为收费版和免费的 community 版本, 具体区别可以参考<a href="https://www.eve-ng.net/index.php/features-compare/">这里</a>; eve-ng 可以在 bare metal, vmware workstation 或者 esxi 上部署. 在虚拟机环境上部署时相当于在 hypervisor 中再嵌套虚拟, 所以性能会有一定影响. 好在 eve-ng 的安装文档非常清晰, 安装时只要按照官网的文档即可. </p>
<p>常用的资源如下:</p>
<ul>
<li><a href="https://www.eve-ng.net/index.php/documentation/installation/">安装eve-ng</a></li>
<li><a href="https://www.eve-ng.net/index.php/documentation/howtos/">上传各厂商的image</a></li>
</ul>
<h2 id="上传镜像"><a href="#上传镜像" class="headerlink" title="上传镜像"></a>上传镜像</h2><p>我们以思科的 vios 和 viosl2 为例来快速演示如何将某个厂商的镜像上传至 eve-ng 以供后期使用. vios 用来模拟 cisco router 而 viosl2 用来模拟 cisco switch.</p>
<p>首先登录 eve-ng 的 ssh 并创建如下文件夹:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir /opt/unetlab/addons/qemu/vios-adventerprisek9-m-15.6.2T</span><br><span class="line">mkdir /opt/unetlab/addons/qemu/viosl2-adventerprisek9-m-15.2.4055</span><br></pre></td></tr></table></figure>

<p>这里需要注意的是 qemu 后的文件夹名称必须要和 image 名称一致并且符合 eve-ng 的<a href="https://www.eve-ng.net/index.php/documentation/qemu-image-namings/">命名规范</a></p>
<p>文件夹创建完毕后将上述两 image 上传到对应的文件夹中. 这里需要注意的是 virtioa.qcow2 是上述两个image 转换后的 image 名称, 需要符合命名规范不能自行修改.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># ls -al ./*</span><br><span class="line">./vios-adventerprisek9-m-15.6.2T:</span><br><span class="line">total 125452</span><br><span class="line">drwxr-xr-x 2 root root      4096 Oct  8 23:04 .</span><br><span class="line">drwxr-xr-x 4 root root      4096 Oct  8 22:58 ..</span><br><span class="line">-rw-r--r-- 1 root root 128450560 Oct  8 23:06 virtioa.qcow2</span><br><span class="line"></span><br><span class="line">./viosl2-adventerprisek9-m-15.2.4055:</span><br><span class="line">total 94600</span><br><span class="line">drwxr-xr-x 2 root root     4096 Oct  8 23:07 .</span><br><span class="line">drwxr-xr-x 4 root root     4096 Oct  8 22:58 ..</span><br><span class="line">-rw-r--r-- 1 root root 96862208 Oct  8 23:08 virtioa.qcow2</span><br></pre></td></tr></table></figure>

<p>上传完毕后在上面两个目录内均需要执行下述命令:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/opt/unetlab/wrappers/unl_wrapper -a fixpermissions</span><br></pre></td></tr></table></figure>

<p>这样就可以在 eve-ng 的 lab 界面中直接拖拽并使用了. 至于其他支持的厂商设备都可以在上述的链接中找到部署方法. </p>
<script type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.7.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.7.0/dist/mindmap.min.css">]]></content>
      <categories>
        <category>eve-ng</category>
      </categories>
      <tags>
        <tag>install</tag>
        <tag>router</tag>
      </tags>
  </entry>
  <entry>
    <title>闲置宽带躺赚钱-还要什么自行车?!</title>
    <url>/cn/pi-docker-earn-money/</url>
    <content><![CDATA[<p>闲置宽带也能赚钱？没听错吧？这年头谁家还没有条宽带？没错！我们今天要说的就是如何充分利用你家里的宽带为你带来收益。</p>
<span id="more"></span>

<p>相信大家感受到近几年来家用宽带的资费越来越低，电信联通移动的良性竞争使得消费者得到了切实的实惠；但是与此同时商用的网络流量需求却出现了井喷式的需求。各种视频网站，网盘云盘你方唱罢我登场，好不热闹。但是问题来了，这些服务供应商如何能把资源又快又好地传输给分布在天南地北的用户们呢？于是网络服务商们不得不在各地建立起大量的数据中心，这样北京的用户可以从北京的数据中心直接获取资源，海南的用户也可以从靠近海南的数据中心直接获取资源。这个方案听上去非常合理但是其背后的运营成本非常之高！</p>
<p>这时候就有大聪明们想到了一个方法：一边是被闲置的家用宽带，而另一边是价格昂贵的数据中心。我们是不是可以利用这些家用宽带为商业网络服务呢？</p>
<p>比如你的邻居今天需要下载一张图片。按照传统的方法他从家里的网络连接到几十，甚至数百公里外的数据中心下载到了这张图片。但是如果这张图片正好在你的电脑里就有，那是不是你可以直接通过网络传输给他？这样他能得到非常好的下载体验，而你也可以为这样的一个“善举”获得网络服务商的回报！而这一切都通过软件在后台自动处理，你甚至没有感觉就获得了服务商发放的收入。</p>
<h1 id="注册"><a href="#注册" class="headerlink" title="注册"></a>注册</h1><p>首先我们需要分别注册这些服务商的账号，稍后完成部署后需要将设备绑定到自己的账号下。</p>
<p>目前这样的软件提供商国外的有：</p>
<p>国内的有：</p>
<ul>
<li><a href="https://www.tiptime.cn/index.html">甜糖</a>（注册时可以填写邀请码：257734，获得15张心愿加成卡可以获得额外10%的回报）</li>
<li><a href="https://act.walk-live.com/acts/invite/v3/?inviteid=d96c8b9f">网心云</a></li>
<li><a href="https://traffmonetizer.com/?aff=408363">TRAFFMONETIZER</a> (这家现在注册就送5美金，可以先注册个账号占位先)</li>
<li><a href="https://p2pr.me/166012971162f391aff3987">PEER2Profit</a></li>
</ul>
<h1 id="选择部署方式"><a href="#选择部署方式" class="headerlink" title="选择部署方式"></a>选择部署方式</h1><p>上面这几家都提供非常丰富的部署方式，比如 windows 电脑，苹果MACOS，安卓手机，平板，电视盒子，docker，群晖，树莓派3b，4b，x86 服务器等。如果你是一个电脑小白不会折腾安装也没关系，国内的几家服务商还提供家用路由器解决方案，你可以直接购买他们旗下的路由器硬件产品。这样的路由器除了负责家里的网络本职工作外，还可以通过预装的软件直接贡献带宽来换取利益。具体的部署方式可以参考</p>
<ul>
<li><a href="https://www.tiptime.cn/green-blog.html">甜糖</a>（注册时可以填写邀请码：257734，获得15张心愿加成卡可以获得额外10%的回报）</li>
<li><a href="https://help.onethingcloud.com/7cb4">网心云</a></li>
<li><a href="https://dashboard.peer2profit.app/download">PEER2Profit</a></li>
<li><a href="https://traffmonetizer.com/?aff=408363">TRAFFMONETIZER</a></li>
</ul>
<p>我们这里选择 Docker，因为 Docker 的稳定性通用性最好，而且可以同时部署多个以上的方案并行，做到利益最大化。下面提供的命令基本只要复制粘贴到运行docker 的主机即可，方便快捷！当然你也可以通过其他方式比如在 windows 上安装软件，安卓手机安装app，又或者在家里的群晖NAS 或者 openwrt 软路由上安装</p>
<h1 id="收益"><a href="#收益" class="headerlink" title="收益"></a>收益</h1><p>有可能你会问这些项目的收益如何？老实说带宽的收益回报并不高，但是它的成本几乎为0。它利用的是你家里本来就被浪费的带宽。而基本都安装在那些 7x24 运行的设备上，比如 NAS 或者 openwrt 路由器。所以如果它可以提供额外的一些补贴又何乐而不为呢？部署完成后只需要让它安静的躺在那里，说不定那天你想起来时能补贴包烟钱呢！另外需要知道的是这类的服务商一般占用的是你的上行带宽，而我们日常需要使用的是下行带宽，所以对于宽带的使用几乎没有影响！</p>
<h1 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h1><h2 id="Docker-安装"><a href="#Docker-安装" class="headerlink" title="Docker 安装"></a>Docker 安装</h2><p>我使用的系统是 ubuntu 22.04。 Docker 的安装非常简单</p>
<ol>
<li>更新系统: <code>sudo apt update &amp;&amp; sudo apt dist-upgrade &amp;&amp; apt-get install -y apt-transport-https</code></li>
<li>安装 docker: <code>curl -sSL get.docker.com | sudo sh</code></li>
<li>配置Docker开机启动: <code>sudo systemctl daemon-reload &amp;&amp; sudo systemctl restart docker</code></li>
<li>重启: <code>reboot</code></li>
</ol>
<h2 id="TRAFFMONETIZER"><a href="#TRAFFMONETIZER" class="headerlink" title="TRAFFMONETIZER"></a><a href="https://traffmonetizer.com/?aff=408363">TRAFFMONETIZER</a></h2><p>将下面 –token 后的字符串替换为你的token，token 在 TRAFFMONETIZER 网站登录后的 Dashboard 中可以找到. 下面的命令根据自己运行 docker 的host 主机类型选择其中一种即可。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"># 64 位 X86 设备</span><br><span class="line">docker run -d --name tm traffmonetizer/cli:latest start accept --token your_token_here</span><br><span class="line"></span><br><span class="line"># 32 位 ARM 设备</span><br><span class="line">docker run -d --name tm traffmonetizer/cli:arm32v7 start accept --token your_token_here</span><br><span class="line"></span><br><span class="line"># 64 位 ARM 设备，比如树莓派 4</span><br><span class="line">docker run -d --name tm traffmonetizer/cli:arm64v8 start accept --token your_token_here</span><br></pre></td></tr></table></figure>

<h2 id="PEER2Profit"><a href="#PEER2Profit" class="headerlink" title="PEER2Profit"></a><a href="https://p2pr.me/166012971162f391aff3987">PEER2Profit</a></h2><p>将 P2P_EMAIL 后的邮件地址替换为自己注册时使用的地址, 无需引号</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -d --restart always -e P2P_EMAIL=yourmail@youmail.com --name peer2profit peer2profit/peer2profit_linux:latest</span><br></pre></td></tr></table></figure>

<h2 id="甜糖"><a href="#甜糖" class="headerlink" title="甜糖"></a><a href="https://www.tiptime.cn/index.html">甜糖</a></h2><p>/mnt/tiantang 是 host 主机上的本地目录，用来存放其缓存文件。如果需要可以自行修改</p>
<p>/mnt/data/ttnode 是 docker 镜像中的目录，无需修改</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -d -v /mnt/tiantang:/mnt/data/ttnode -v /var/run/docker.sock:/var/run/docker.sock --name ttnode --hostname ttnode --net=host --restart=always --memory=2g tiptime/ttnode:latest</span><br></pre></td></tr></table></figure>

<h2 id="网心云"><a href="#网心云" class="headerlink" title="网心云"></a><a href="https://act.walk-live.com/acts/invite/v3/?inviteid=d96c8b9f">网心云</a></h2><p>/mnt/wxedge_storage 是 host 主机上的本地目录，用来存放其缓存文件。如果需要可以自行修改</p>
<p>/storage:rw 是 docker 镜像中的目录，无需修改</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run --name=wxedge --restart=always --privileged --net=host --tmpfs /run --tmpfs /tmp -v /mnt/wxedge_storage:/storage:rw -d registry.hub.docker.com/onething1/wxedge</span><br></pre></td></tr></table></figure>

<p>上述几条命令可以非常快速地架设起节点，几分钟你家的宽带就开始为你赚钱啦！</p>
<h1 id="网络环境"><a href="#网络环境" class="headerlink" title="网络环境"></a>网络环境</h1><p>另外需要提醒的是家中的宽带一般都需要打开路由器上的 UPNP 功能，这样才能让网络流量可以穿越路由器到达这些docker 节点中。具体打开的方法根据路由器型号，品牌的不同也多种多样。大家可以自行在网上查找</p>
<p>并且如果需要在一台设备上跑多个 docker 实例，还需将 docker 的网络配置为 macvlan 的模式：</p>
<p>将网卡设置为混杂模式：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">crontab -e</span><br><span class="line"></span><br><span class="line">@reboot /usr/sbin/ifconfig eth0 promisc</span><br></pre></td></tr></table></figure>

<p>raspberry 的话安装包: </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apt install linux-modules-extra-raspi</span><br></pre></td></tr></table></figure>

<p>创建 macvlan 网络，192.168.1.0/24 可以理解为家里的网络网段</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker network create -d macvlan --subnet=192.168.1.0/24 --gateway=192.168.1.254 -o parent=eth0 -o macvlan_mode=bridge macnet</span><br></pre></td></tr></table></figure>

<p>在 macvlan 网络中创建镜像:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run --name=wxedge --restart=always --privileged=true --net=macnet --ip=192.168.1.100 --tmpfs /run --tmpfs /tmp -v /mnt/wxedge_storage:/storage:rw -d registry.hub.docker.com/onething1/wxedge</span><br><span class="line">docker run -d -v /mnt/tiantang:/mnt/data/ttnode -v /var/run/docker.sock:/var/run/docker.sock --name ttnode --hostname ttnode --net=macnet --ip=192.168.1.101 --privileged=true --restart=always tiptime/ttnode:latest</span><br><span class="line">docker run -d --restart always --net=macnet --ip=192.168.1.102 --privileged=true --name tm traffmonetizer/cli:arm64v8 start accept --token xxx</span><br><span class="line">docker run -d --restart always --net=macnet --ip=192.168.1.103 --privileged=true -e P2P_EMAIL=xxx --name peer2profit peer2profit/peer2profit_linux:latest</span><br></pre></td></tr></table></figure><script type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.7.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.7.0/dist/mindmap.min.css">]]></content>
      <categories>
        <category>docker</category>
        <category>cdn</category>
      </categories>
      <tags>
        <tag>网心云</tag>
        <tag>甜糖</tag>
        <tag>traffmonetizer</tag>
        <tag>peer2profit</tag>
        <tag>raspberry pi 4b</tag>
      </tags>
  </entry>
  <entry>
    <title>树莓派踩坑指南之外挂硬盘</title>
    <url>/cn/pi-hard-disk/</url>
    <content><![CDATA[<p>树莓派 4b 通过 USB 外挂硬盘经常会遇到各种奇葩问题，比如：硬盘可以识别但重启后无法识别；/etc/fstab 挂载硬盘重启后树莓派无法登录；硬盘挂载后时不时掉线，需要手动挂载等问题。这些可能都和硬盘盒的 UAS 有关。。。</p>
<span id="more"></span>

<p>解决方法：</p>
<ol>
<li>查看自己硬盘的信息. 其中 JMS578 就是系统识别的 USB 硬盘</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">root@ubuntu:~# lsusb</span><br><span class="line">Bus 003 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub</span><br><span class="line">Bus 002 Device 002: ID 0080:a001 Unknown JMS578 based SATA bridge</span><br><span class="line">Bus 002 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub</span><br><span class="line">Bus 001 Device 002: ID 2109:3431 VIA Labs, Inc. Hub</span><br><span class="line">Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>编辑 /boot/cmdline.txt. 这个文件根据系统安装方式的不同，有可能需要将 tf 卡直接挂载到电脑上才能进行读取编辑。</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">usb-storage.quirks=0080:a001:u console=serial0,115200 console=tty1 root=PARTUUID=dc77c2e2-02 rootfstype=ext4 elevator=deadline fsck.repair=yes rootwait quiet splash plymouth.ignore-serial-consoles</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>完成后重启再查看问题是否解决。另外在 /etc/fstab 中挂载磁盘时建议加入 nofail 参数，这样在磁盘挂载出现问题时也不至于无法访问系统。</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">UUID=adc0b8fe-2b19-4954-99bc-069e7a3461c2 /mnt ext4 defaults,nofail 0 0</span><br></pre></td></tr></table></figure><script type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.7.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.7.0/dist/mindmap.min.css">]]></content>
      <categories>
        <category>raspberry pi</category>
      </categories>
      <tags>
        <tag>硬盘</tag>
        <tag>quirk</tag>
      </tags>
  </entry>
  <entry>
    <title>初试 oh-my-zsh</title>
    <url>/cn/ohmyzsh/</url>
    <content><![CDATA[<p>最近发现越来越多的同学都转向了zsh。其实 bash 用了好多年也并没有发现不顺手的地方。不过zsh貌似支持许多很酷的第三方插件，所以先记录下在 macos 中如何快速切换到 zsh 环境。</p>
<span id="more"></span>

<p>安装：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">brew install zsh</span><br></pre></td></tr></table></figure>

<p>改变shell：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">chsh -s /bin/zsh</span><br></pre></td></tr></table></figure>

<p>安装 oh-my-zsh：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sh -c &quot;$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)&quot;</span><br></pre></td></tr></table></figure>

<p>配置 oh-my-zsh:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vi ~/.zshrc</span><br></pre></td></tr></table></figure>
<p>找到以下内容添加 zsh-autosuggestions 和 zsh-syntax-highlighting 的插件支持</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">plugins=(git macos zsh-autosuggestions zsh-syntax-highlighting)</span><br></pre></td></tr></table></figure>

<p>继续在 .zshrc 添加如下配置, 添加配置后可以在 “$HOME/zsh/aliasrc” 文件中添加需要的命令 alias：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">HIST_STAMPS=&quot;yyyy-mm-dd&quot;</span><br><span class="line">HISTSIZE=10000</span><br><span class="line">SAVEHIST=10000</span><br><span class="line">HISTFILE=~/.cache/zshhistory</span><br><span class="line"></span><br><span class="line"># Load aliases and shortcuts if existent.</span><br><span class="line">[ -f &quot;$HOME/zsh/aliasrc&quot; ] &amp;&amp; source &quot;$HOME/zsh/aliasrc&quot;</span><br></pre></td></tr></table></figure>



<p>安装插件：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 这个插件可以自动提示之前输入的命令</span><br><span class="line">git clone git://github.com/zsh-users/zsh-autosuggestions $ZSH_CUSTOM/plugins/zsh-autosuggestions</span><br><span class="line"># 这个插件可以启用语法高亮</span><br><span class="line">git clone git://github.com/zsh-users/zsh-syntax-highlighting $ZSH_CUSTOM/plugins/zsh-syntax-highlighting</span><br></pre></td></tr></table></figure>

<p>应用配置：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">source ~/.zshrc</span><br></pre></td></tr></table></figure><script type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.7.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.7.0/dist/mindmap.min.css">]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>zsh</tag>
      </tags>
  </entry>
  <entry>
    <title>Pod 与 Service</title>
    <url>/cn/pod-and-service/</url>
    <content><![CDATA[<p>对于 Kubernetes (K8S) 的初学者来说最早接触到的一个概念可能就是 Pod 与 Service. Pod 简单来说就是一个或者一组 container 的集合. 但是在 K8S 的世界里, Pod 的一生可能非常短暂, 稍纵即逝. 而且 Pod 在每次启动时都会分配一个新的 IP 地址. 这样的话集群中互相依赖的 Pod 之间的访问就成了一个新的问题!!</p>
<span id="more"></span>

<p>K8S 为了解决这个问题引入了 Service 资源. Service 资源的本质是为一组 Pod 提供一个相对固定的 IP 地址. 无论 Pod 的 IP 地址如何变化, Service 的 IP 地址都不会改变. K8S 集群中每个 Node 上运行的 kube-proxy 组件负责转发 Service 与 Pod 之间的流量. kube-proxy 通过 iptables DNAT 或者 ipvs 规则将一个数据包的目的IP 从 Service 的 Cluster IP 修改为其背后的 Pod 的 IP 地址并进行转发. 比如在我们的 K8S 集群中有如下 pod:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># kubectl get pod dvwa-6bcb999d58-hpg2j -o wide</span><br><span class="line">NAME                    READY   STATUS    RESTARTS   AGE   IP            NODE    NOMINATED NODE   READINESS GATES</span><br><span class="line">dvwa-6bcb999d58-hpg2j   1/1     Running   0          29d   10.42.3.125   node2   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure>

<p>我们看到这个 Pod 的 IP 地址为 10.42.3.125, 并且运行在 node2 这个节点上. 这个 Pod 对应的 Service 为:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># kubectl get service dvwa -o wide</span><br><span class="line">NAME   TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE   SELECTOR</span><br><span class="line">dvwa   NodePort   10.43.18.128   &lt;none&gt;        80:30237/TCP   29d   app=dvwa</span><br></pre></td></tr></table></figure>

<p>我们看到这个 Service 的 CLUSTER-IP 为 10.43.18.128. 我们尝试访问下这个服务, 可以看到请求可以正常转发到实际通过服务的 Pod 上.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># curl -I http://10.43.18.128/login.php</span><br><span class="line">HTTP/1.1 200 OK</span><br><span class="line">Date: Fri, 03 Sep 2021 13:22:12 GMT</span><br><span class="line">Server: Apache/2.4.25 (Debian)</span><br><span class="line">Set-Cookie: PHPSESSID=reqgk2qhhcmf77m9l6fhokftb5; path=/</span><br><span class="line">Expires: Tue, 23 Jun 2009 12:00:00 GMT</span><br><span class="line">Cache-Control: no-cache, must-revalidate</span><br><span class="line">Pragma: no-cache</span><br><span class="line">Set-Cookie: PHPSESSID=reqgk2qhhcmf77m9l6fhokftb5; path=/</span><br><span class="line">Set-Cookie: security=low</span><br><span class="line">Content-Type: text/html;charset=utf-8</span><br></pre></td></tr></table></figure>

<p>此时我们重启 node2 再对 Pod 进行观察. 由于 node2 无法提供服务, pod 漂移到了 node1 并且获得了新的 IP 10.42.2.140. 于此同时, K8S 删除了之前的 Pod dvwa-6bcb999d58-hpg2j. 与此同时 Service dvwa 的 IP 地址并没有改变, 访问此IP 仍然可以访问服务:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># kubectl get pod -o wide</span><br><span class="line">NAME                    READY   STATUS        RESTARTS   AGE     IP            NODE    NOMINATED NODE   READINESS GATES</span><br><span class="line">dvwa-6bcb999d58-7n88s   1/1     Running       0          2m58s   10.42.2.140   node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">dvwa-6bcb999d58-hpg2j   1/1     Terminating   1          29d     10.42.3.131   node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"></span><br><span class="line"># kubectl get service dvwa -o wide</span><br><span class="line">NAME   TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE   SELECTOR</span><br><span class="line">dvwa   NodePort   10.43.18.128   &lt;none&gt;        80:30237/TCP   29d   app=dvwa</span><br><span class="line"></span><br><span class="line"># curl -I http://10.43.18.128/login.php</span><br><span class="line">HTTP/1.1 200 OK</span><br><span class="line">Date: Fri, 03 Sep 2021 14:09:26 GMT</span><br><span class="line">Server: Apache/2.4.25 (Debian)</span><br><span class="line">Set-Cookie: PHPSESSID=gvhvtmasg3imm7oe4gqov30pm6; path=/</span><br><span class="line">Expires: Tue, 23 Jun 2009 12:00:00 GMT</span><br><span class="line">Cache-Control: no-cache, must-revalidate</span><br><span class="line">Pragma: no-cache</span><br><span class="line">Set-Cookie: PHPSESSID=gvhvtmasg3imm7oe4gqov30pm6; path=/</span><br><span class="line">Set-Cookie: security=low</span><br><span class="line">Content-Type: text/html;charset=utf-8</span><br></pre></td></tr></table></figure>

<p>通过 iptables 我们还可以来查看下 kube-proxy 定义的数据包转发规则. 首先我们可以找到名为 KUBE-SERVICES 的 chain:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># iptables -t nat -nvL PREROUTING | grep KUBE</span><br><span class="line">  39M 8368M KUBE-SERVICES  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service portals */</span><br></pre></td></tr></table></figure>

<p>在 KUBE-SERVICES 这条 chain 里, 我们可以找到与我们 Service DVWA 相关的两条规则:</p>
<ul>
<li>KUBE-MARK-MASQ 会对所有发往 Service 10.43.18.128 80 端口的流量进行标记. 所有被标记的数据包都会在 POSTROUTING 规则中 SNAT 源地址为 node 的 IP 地址.</li>
<li>KUBE-SVC-DVCPEHURWBFNLMQF 会匹配到所有发往 10.43.18.128 这个 IP 地址 80 端口的流量.</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># iptables -t nat -nvL KUBE-SERVICES | grep dvwa</span><br><span class="line">    0     0 KUBE-MARK-MASQ  tcp  --  *      *      !10.42.0.0/16         10.43.18.128         /* default/dvwa:http cluster IP */ tcp dpt:80</span><br><span class="line">    0     0 KUBE-SVC-DVCPEHURWBFNLMQF  tcp  --  *      *       0.0.0.0/0            10.43.18.128         /* default/dvwa:http cluster IP */ tcp dpt:80</span><br></pre></td></tr></table></figure>

<p>最终我们顺着 chain 可以找到 KUBE-SEP-7NAXZ4TOZS5VAIRS. 里面的DNAT 规则将所有流量都转发到 10.42.2.140 的80端口.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># iptables -t nat -nvL KUBE-SVC-DVCPEHURWBFNLMQF</span><br><span class="line">Chain KUBE-SVC-DVCPEHURWBFNLMQF (2 references)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line">    0     0 KUBE-SEP-7NAXZ4TOZS5VAIRS  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/dvwa:http */</span><br><span class="line"># iptables -t nat -nvL KUBE-SEP-7NAXZ4TOZS5VAIRS</span><br><span class="line">Chain KUBE-SEP-7NAXZ4TOZS5VAIRS (1 references)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line">    0     0 KUBE-MARK-MASQ  all  --  *      *       10.42.2.140          0.0.0.0/0            /* default/dvwa:http */</span><br><span class="line">    0     0 DNAT       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/dvwa:http */ tcp to:10.42.2.140:80</span><br></pre></td></tr></table></figure>

<p>还记得 10.42.2.140 是谁的 IP 地址吗? </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># kubectl get pod -o wide</span><br><span class="line">NAME                    READY   STATUS        RESTARTS   AGE     IP            NODE    NOMINATED NODE   READINESS GATES</span><br><span class="line">dvwa-6bcb999d58-7n88s   1/1     Running       0          2m58s   10.42.2.140   node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">dvwa-6bcb999d58-hpg2j   1/1     Terminating   1          29d     10.42.3.131   node2   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure>

<p>所以通过上面的实验我们可以大致理解下一个数据包如何通过 iptables 的转发最终到达 Pod. </p>
<script type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.7.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.7.0/dist/mindmap.min.css">]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>pod</tag>
        <tag>service</tag>
        <tag>iptables</tag>
      </tags>
  </entry>
</search>
